[
  {
    "step_no": 1,
    "topic": "Introduction to OS",
    "data": [
      {
        "id": "OS-and-main-functions",
        "sl_no_in_step": 1,
        "title": "OS and main functions",
        "yt_link": "https://youtube.com/playlist?list=PLrL_PSQ6q060N-0DKaG3HYIg3dfgSLPp7&si=LbMPtIp6v3Ni7uFM",
        "article_link": "https://takeuforward.org/operating-system/os-and-main-functions",
        "content": " <p>A computer is made up of two parts: software programs and the hardware they run on, like electronic chips and the motherboard. The operating system connects these two parts, providing a platform for software to interact with hardware.</p>\n\n    <p>The operating system acts as a manager, coordinating and controlling the computer's resources. Whenever you send a command, press a key, or move the mouse, the operating system ensures the right changes happen on the hardware side.</p>\n\n<img src=\" https://static.takeuforward.org/content/-cRpoQymL\", width=400>\n\n    <h3>Purpose and Functions of an Operating System</h3>\n    <ul>\n        <li><b>Process Manager:</b> Manages the programs running on the computer, ensuring they get enough CPU time and can communicate properly with input/output devices.</li>\n        <li><b>Memory Manager:</b> Allocates and deallocates memory for each process, allowing multiple processes to run simultaneously without interruption.</li>\n        <li><b>Device Manager:</b> Controls communication between hardware devices (like keyboards and printers) and the software commands you run.</li>\n        <li><b>File Manager:</b> Organizes and provides access to files, folders, and directories on your computer and external storage devices.</li>\n        <li><b>User Interface:</b> Provides a way for users to interact with the computer, either through a graphical interface or a command line.</li>\n    </ul>\n\n    <h3>What Happens When You Execute a Command?</h3>\n    <p>When you execute a command (like clicking the mouse or pressing a key), several processes occur:</p>\n<img src=\"https://static.takeuforward.org/content/-knYagrnN\", width=400>\n    <ol>\n        <li>The <b>Device Manager</b> detects the electrical signal from your action.</li>\n        <li>The <b>Process Manager</b> acknowledges that the command has been received.</li>\n        <li>The <b>Process Manager</b> determines what resources the command needs (like memory or external devices).</li>\n        <li>If necessary, it calculates the file location on disk and communicates with the <b>Device Manager</b> to retrieve it.</li>\n        <li>The required files are loaded into the <b>RAM</b> so the <b>CPU</b> can execute the program.</li>\n        <li>After execution, the <b>Process Manager</b> sends outputs to the appropriate managers to display results or produce sounds.</li>\n        <li>If needed, outputs are stored in secondary memory via the <b>File Manager</b>.</li>\n    </ol>\n\n    <h3>Examples of Popular Operating Systems</h3>\n\n    <ul>\n        <li>\n            <b>Windows:</b> Developed by Microsoft, it is the most widely used operating system for personal computing, evolving from Windows 1.0 to Windows 10.</li>\n<img src=\"https://static.takeuforward.org/content/-RThSaCI5\" alt=\"Windows OS\", width=400>\n\n        <li>\n            <b>macOS:</b> A proprietary operating system designed for Apple computers, known for its user-friendly interface and integration with Apple hardware.</li>\n               <img src=\"https://static.takeuforward.org/content/-d2g490CW\" alt=\"macOS\", width=400>\n        <li>\n            <b>Linux:</b> An open-source operating system kernel, developed by Linus Torvalds, used in various devices from servers to embedded systems. It has several distributions like Ubuntu and Fedora.</li>\n<img src=\"https://static.takeuforward.org/content/-wjUF134k\" alt=\"Linux\", width=400>\n\n    </ul>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Process-Program-and-Thread",
        "sl_no_in_step": 2,
        "title": "Process Program and Thread",
        "yt_link": "https://youtu.be/DUlgdZVgCNI?si=HlRcojvTwqL8UIL1",
        "article_link": "https://takeuforward.org/operating-system/batch-os",
        "content": "    <p><b>Batch Operating Systems</b> are computing environments where tasks are processed in batches, without the need for immediate user interaction. The jobs are submitted to the system one at a time, and each job is executed sequentially. This means that once a job begins processing, no other job can start until the current job is finished executing.</p>\n\n    <p><b>Batch Operating Systems</b> were developed in the early days of mainframe computers and have evolved over time to accommodate modern computing needs.</p>\n\n    <h3>Batch Processing System Architecture</h3>\n    <img src=\"https://static.takeuforward.org/content/-X4CwE0aG\" alt=\"Batch Processing System Architecture Diagram\">\n    \n    <h3>Components:</h3>\n    <ul>\n        <li><b>Job Queue:</b> A queue where incoming jobs are stored until they are ready for processing.</li>\n        <li><b>Job Scheduler:</b> Responsible for determining the order in which jobs are executed and allocating system resources accordingly.</li>\n        <li><b>Job Control Language (JCL):</b> A specialised language used to define and control batch processing jobs.</li>\n        <li><b>Batch Monitor:</b> Software that oversees the execution of batch jobs and manages system resources.</li>\n    </ul>\n\n    <h3>Advantages of Batch Processing</h3>\n    <ul>\n        <li><b>Efficiency:</b> Batch processing allows for the efficient processing of large volumes of data by grouping tasks into batches.</li>\n        <li><b>Optimised Resource Use:</b> By scheduling jobs in advance and optimising resource allocation, batch processing systems can make efficient use of available resources.</li>\n        <li><b>Automation:</b> Batch processing automates repetitive tasks, reducing the need for manual intervention and increasing productivity.</li>\n        <li><b>Error Handling:</b> Batch processing systems can incorporate error handling mechanisms to detect and address issues that arise during job execution.</li>\n    </ul>\n\n    <h3>Disadvantages of Batch Processing</h3>\n    <ul>\n        <li><b>Not Suitable for Real-Time Processing:</b> Batch processing systems are not well-suited for real-time processing requirements, as jobs are executed sequentially and may experience delays.</li>\n        <li><b>Longer Response Times:</b> Because jobs are processed in batches, users may experience longer response times compared to interactive processing systems.</li>\n        <li><b>Lack of Flexibility:</b> Batch processing systems rely on predefined schedules for job execution, which may not be flexible enough to accommodate changing requirements or dynamic workloads.</li>\n        <li><b>Limited Interactivity:</b> Batch processing systems lack the interactivity of interactive processing systems, making them less suitable for tasks that require immediate user feedback or interaction.</li>\n    </ul>\n\n    <h3>Examples of Batch Processing Systems</h3>\n    <ul>\n        <li><b>Payroll Processing Systems:</b> These systems calculate employee salaries and process payroll transactions in batches, typically on a regular schedule (e.g., weekly or monthly).</li>\n        <li><b>Report Generation Systems:</b> Systems that generate reports based on data collected from various sources, such as sales reports, inventory reports, and financial statements.</li>\n        <li><b>Data Processing Systems:</b> Systems that process large volumes of data, such as data cleansing, transformation, and aggregation, in batches to generate insights and support decision-making.</li>\n        <li><b>Automated Backup Systems:</b> Backup systems that automatically back up data from multiple sources to storage devices at scheduled intervals, ensuring data integrity and availability.</li>\n    </ul>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>\n"
      },
      {
        "id": "Types-of-OS",
        "sl_no_in_step": 3,
        "title": "Types of OS",
        "yt_link": "https://youtu.be/rphayBa3g5U?si=2b5VD9QI_Nk6edvY",
        "article_link": "https://takeuforward.org/operating-system/types-of-OS",
        "content": "<p>Operating Systems can fall into various categories based on their speed of response, methods used to enter data into the system, and internal process execution.</p>\n\n    <h4>1. Batch Operating Systems</h4>\n    <p>Batch Operating Systems execute processes as complete batches, where jobs are entered one at a time. Once a job begins processing, no other job can start until the current job finishes executing. These jobs are executed sequentially without user interaction.</p>\n    <p>They are designed to process large volumes of data efficiently and automate repetitive tasks such as payroll processing, report generation, and data processing.</p>\n    <p>The efficiency of a batch system is measured in throughput, which is the number of jobs completed in a given amount of time.</p>\n    <p><img src=\"https://static.takeuforward.org/content/-UsOPGwGK\" alt=\"Batch Operating System\" width=\"400\"></p>\n\n    <h4>2. Multiprogramming Operating Systems</h4>\n    <p>Multiprogramming Operating Systems allow multiple programs to run simultaneously by sharing the CPU and other system resources. These systems quickly switch between programs, giving the appearance that all programs are executing at the same time.</p>\n    <p>This increases CPU utilization and throughput, allowing the system to handle tasks more efficiently and improve response time and user productivity by reducing idle time.</p>\n    <p><img src=\"https://static.takeuforward.org/content/-bwZWiOHp\" alt=\"Multiprogramming Operating System\" width=\"400\"></p>\n\n    <h4>3. Real-Time Operating Systems</h4>\n    <p>Real-Time Operating Systems (RTOS) are used in time-critical environments where reliability and deadlines are important. They are designed to guarantee a timely response to input events within a specified time frame.</p>\n    <p>These are used in applications that require deterministic behavior and strict timing constraints, such as process control, robotics, avionics, and medical devices.</p>\n\n    <h4>4. Distributed Operating Systems</h4>\n    <p>Distributed Operating Systems manage resources and coordinate tasks across multiple interconnected computers and nodes. They support distributed file systems, process management, and communication protocols across nodes.</p>\n    <p>This provides the user an interface to hide the underlying complexity and a simple way to execute programs across multiple machines and storage devices.</p>\n    <p><img src=\"https://static.takeuforward.org/content/-a4d4y1nu\" alt=\"Distributed Operating System\" width=\"400\"></p>\n\n    <h4>5. Embedded Operating Systems</h4>\n    <p>Embedded Operating Systems are specialized operating systems designed to run on embedded systems, which are computer chips embedded into larger devices or machinery. These systems are resource-constrained and optimized for specific tasks and applications.</p>\n    <p>They are used in a wide range of embedded devices across various industries, including consumer electronics, automotive systems, industrial automation, medical devices, and IoT (Internet of Things) devices.</p>\n\n    <h3>Applications of Different Types of Operating Systems</h3>\n    <table border=\"1\">\n        <thead>\n            <tr>\n                <th>Operating System Type</th>\n                <th>Application</th>\n                <th>Example</th>\n            </tr>\n        </thead>\n        <tbody>\n            <tr>\n                <td>Batch OS</td>\n                <td>Payroll Processing, Report Generation, Data Processing</td>\n                <td>IBM z/OS</td>\n            </tr>\n            <tr>\n                <td>Multiprogramming OS</td>\n                <td>Time Sharing Systems, Web Servers, Database Servers</td>\n                <td>Linux, macOS, Windows NT, MySQL, Nginx</td>\n            </tr>\n            <tr>\n                <td>Real-Time OS</td>\n                <td>Process Control, Robotics, Aerospace Systems</td>\n                <td>VxWorks, RTLinux, ROS</td>\n            </tr>\n            <tr>\n                <td>Distributed OS</td>\n                <td>Distributed File System, Cloud Computing, Peer-to-peer networks</td>\n                <td>Google’s File System, Apache Hadoop, BitTorrent</td>\n            </tr>\n            <tr>\n                <td>Embedded OS</td>\n                <td>Consumer Electronics, Automotive Systems, Industrial Automation, Internet of Things (IoT) devices</td>\n                <td>FreeRTOS, Contiki, AUTOSAR (Automotive Open System Architecture), QNX, Android (used in smartphones), iOS (used in iPhones)</td>\n            </tr>\n        </tbody>\n    </table>\n    <blockquote class=\"wp-block-quote is-style-default\">\n        <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p>\n    </blockquote>\n"
      },
      {
        "id": "Batch-OS",
        "sl_no_in_step": 4,
        "title": "Batch OS",
        "yt_link": "https://youtu.be/rphayBa3g5U?si=2b5VD9QI_Nk6edvY",
        "article_link": "https://takeuforward.org/operating-system/types-of-OS",
        "content": "<p>Operating Systems can fall into various categories based on their speed of response, methods used to enter data into the system, and internal process execution.</p>\n\n    <h4>1. Batch Operating Systems</h4>\n    <p>Batch Operating Systems execute processes as complete batches, where jobs are entered one at a time. Once a job begins processing, no other job can start until the current job finishes executing. These jobs are executed sequentially without user interaction.</p>\n    <p>They are designed to process large volumes of data efficiently and automate repetitive tasks such as payroll processing, report generation, and data processing.</p>\n    <p>The efficiency of a batch system is measured in throughput, which is the number of jobs completed in a given amount of time.</p>\n    <p><img src=\"https://static.takeuforward.org/content/-UsOPGwGK\" alt=\"Batch Operating System\" width=\"400\"></p>\n\n    <h4>2. Multiprogramming Operating Systems</h4>\n    <p>Multiprogramming Operating Systems allow multiple programs to run simultaneously by sharing the CPU and other system resources. These systems quickly switch between programs, giving the appearance that all programs are executing at the same time.</p>\n    <p>This increases CPU utilization and throughput, allowing the system to handle tasks more efficiently and improve response time and user productivity by reducing idle time.</p>\n    <p><img src=\"https://static.takeuforward.org/content/-bwZWiOHp\" alt=\"Multiprogramming Operating System\" width=\"400\"></p>\n\n    <h4>3. Real-Time Operating Systems</h4>\n    <p>Real-Time Operating Systems (RTOS) are used in time-critical environments where reliability and deadlines are important. They are designed to guarantee a timely response to input events within a specified time frame.</p>\n    <p>These are used in applications that require deterministic behavior and strict timing constraints, such as process control, robotics, avionics, and medical devices.</p>\n\n    <h4>4. Distributed Operating Systems</h4>\n    <p>Distributed Operating Systems manage resources and coordinate tasks across multiple interconnected computers and nodes. They support distributed file systems, process management, and communication protocols across nodes.</p>\n    <p>This provides the user an interface to hide the underlying complexity and a simple way to execute programs across multiple machines and storage devices.</p>\n    <p><img src=\"https://static.takeuforward.org/content/-a4d4y1nu\" alt=\"Distributed Operating System\" width=\"400\"></p>\n\n    <h4>5. Embedded Operating Systems</h4>\n    <p>Embedded Operating Systems are specialized operating systems designed to run on embedded systems, which are computer chips embedded into larger devices or machinery. These systems are resource-constrained and optimized for specific tasks and applications.</p>\n    <p>They are used in a wide range of embedded devices across various industries, including consumer electronics, automotive systems, industrial automation, medical devices, and IoT (Internet of Things) devices.</p>\n\n    <h3>Applications of Different Types of Operating Systems</h3>\n    <table border=\"1\">\n        <thead>\n            <tr>\n                <th>Operating System Type</th>\n                <th>Application</th>\n                <th>Example</th>\n            </tr>\n        </thead>\n        <tbody>\n            <tr>\n                <td>Batch OS</td>\n                <td>Payroll Processing, Report Generation, Data Processing</td>\n                <td>IBM z/OS</td>\n            </tr>\n            <tr>\n                <td>Multiprogramming OS</td>\n                <td>Time Sharing Systems, Web Servers, Database Servers</td>\n                <td>Linux, macOS, Windows NT, MySQL, Nginx</td>\n            </tr>\n            <tr>\n                <td>Real-Time OS</td>\n                <td>Process Control, Robotics, Aerospace Systems</td>\n                <td>VxWorks, RTLinux, ROS</td>\n            </tr>\n            <tr>\n                <td>Distributed OS</td>\n                <td>Distributed File System, Cloud Computing, Peer-to-peer networks</td>\n                <td>Google’s File System, Apache Hadoop, BitTorrent</td>\n            </tr>\n            <tr>\n                <td>Embedded OS</td>\n                <td>Consumer Electronics, Automotive Systems, Industrial Automation, Internet of Things (IoT) devices</td>\n                <td>FreeRTOS, Contiki, AUTOSAR (Automotive Open System Architecture), QNX, Android (used in smartphones), iOS (used in iPhones)</td>\n            </tr>\n        </tbody>\n    </table>\n    <blockquote class=\"wp-block-quote is-style-default\">\n        <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p>\n    </blockquote>\n"
      },
      {
        "id": "Multiprogramming-and-Multitasking-OS",
        "sl_no_in_step": 5,
        "title": "Multiprogramming and Multitasking OS",
        "yt_link": "https://youtu.be/GoKZo9Inc9Q?si=G8UxTLeOOkpA8_tR",
        "article_link": "https://takeuforward.org/operating-system/multiprogramming-multitasking-in-os",
        "content": "    <p>In a <b>Multiprogramming Operating System</b>, CPU utilisation is maximised by keeping multiple jobs in <b>main memory</b> simultaneously. If one job is occupied with <b>Input/Output (I/O) operations</b>, the CPU is assigned to another job.</p>\n    <p>Multiple jobs are organised in the main memory, and the CPU <b>switches between jobs</b> as needed. This approach reduces CPU idle time and increases overall system efficiency.</p>\n    <img src=\"https://static.takeuforward.org/content/-J5m__-dr\" alt=\"Multiprogramming OS Architecture\">\n\n    <h3>Multitasking Operating System</h3>\n    <p>In a <b>Multitasking Operating System</b>, multiple tasks share common resources like the CPU and memory. The tasks are loaded into the CPU and executed by <b>switching rapidly</b> among them using a <b>small time quantum</b>. This switching occurs so quickly that users feel they are interacting with all tasks at the same time.</p>\n    <img src=\"https://static.takeuforward.org/content/-gzRW8QxH\" alt=\"Multitasking OS Diagram\">\n\n    <h3>Benefits of Multiprogramming and Multitasking</h3>\n    <ul>\n        <li><p><b>Higher CPU Utilisation:</b> By allowing the CPU to switch between executing multiple programs or tasks, the system minimises idle time and maximises throughput.</p></li>\n        <li><p><b>Improved Responsiveness:</b> Users experience better responsiveness as multiple tasks can run concurrently, reducing waiting times and creating a smoother user experience.</p></li>\n        <li><p><b>Fault Isolation:</b> Programs or tasks run in <b>separate memory spaces</b>, so if one encounters an error or crashes, others remain unaffected, leading to greater reliability and stability.</p></li>\n        <li><p><b>Efficient Use of Hardware:</b> Sharing hardware resources among multiple programs reduces the need for extra hardware and supports scalability, allowing systems to handle growing workloads efficiently.</p></li>\n    </ul>\n\n    <h3>Challenges and Limitations</h3>\n    <ul>\n        <li><p><b>Resource Contention:</b> Multiple programs or tasks may compete for shared resources like CPU, memory, and I/O devices, leading to <b>resource starvation</b> for some programs.</p></li>\n        <li><p><b>Overhead of Context Switching:</b> The process of saving and restoring the state of a task during a switch consumes CPU cycles and resources. Increased context switching can negatively impact system performance.</p></li>\n        <li><p><b>Complex Synchronisation:</b> Managing concurrent access to shared resources and maintaining data consistency is difficult. Without proper mechanisms, issues like <b>race conditions</b> and <b>deadlocks</b> can occur.</p></li>\n        <li><p><b>Scalability Challenges:</b> Managing thousands or millions of concurrent tasks is hard. As systems grow, ensuring performance, reliability, and scalability becomes more complex.</p></li>\n    </ul>\n\n    <h3>Context Switching</h3>\n    <p><b>Context Switching</b> is the process of saving the state of a process or thread so that it can be paused and resumed later, allowing the CPU to switch between multiple tasks efficiently. It involves storing and restoring information like CPU registers and memory pointers.</p>\n    <p>Context switching enables multitasking operating systems to handle multiple tasks concurrently, creating the illusion of simultaneous execution.</p>\n    <img src=\"https://static.takeuforward.org/content/-2MYodh_c\" alt=\"Context Switching Diagram\">\n\n    <h3>Use Cases of Multiprogramming and Multitasking</h3>\n    <ul>\n        <li><p><b>Example 1 - User Applications:</b> A user can run a word processor while listening to music and downloading files. Each application runs as a separate task, and the OS manages their execution efficiently.</p></li>\n        <li><p><b>Example 2 - Servers:</b> A web server uses multitasking to handle multiple user requests at the same time, ensuring each client gets a response without delay.</p></li>\n        <li><p><b>Example 3 - Background Processes:</b> System updates or backups run in the background while a user continues working on other applications, thanks to the multitasking capabilities of the OS.</p></li>\n    </ul>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Kernel-and-user-modes",
        "sl_no_in_step": 6,
        "title": "Kernel and user modes",
        "yt_link": "https://youtu.be/fc0QZdYYkI4?si=Hv5mhCfet1CUAIEr",
        "article_link": "https://takeuforward.org/operating-system/kernel-user-models",
        "content": "    <p>The <b>kernel</b> is the core component of the operating system that acts as a bridge between the hardware and software layers of a computer system. It is responsible for managing system resources, such as CPU, memory, and Input/Output devices, and providing essential services to run user applications.</p>\n    <p>As the central control program, the kernel governs the operations of the entire operating system.</p>\n    <img src=\"https://static.takeuforward.org/content/-6vRx6G_t\" alt=\"Kernel Overview\", width=400>\n\n    <h3>Kernel Mode</h3>\n    <p>The kernel operates in <b>privileged mode</b>, allowing it to access hardware resources and perform critical system-level tasks. It interacts directly with the hardware components of the computer and provides a layer of abstraction to shield user applications from the complexities of hardware management.</p>\n\n    <h3>User Mode</h3>\n    <p><b>User Mode</b> is a restricted execution environment where user applications and processes run. In User Mode, programs have limited access to system resources and cannot perform privileged operations, such as accessing hardware or modifying system settings.</p>\n    <p>User Mode provides a protected environment that isolates user applications from the critical components of the operating system, ensuring system stability and security. User mode programs communicate with the kernel through controlled mechanisms, such as <b>system calls</b>, to request access to shared resources or perform privileged operations.</p>\n\n    <h3>Privileged Mode</h3>\n    <p><b>Privileged Mode</b>, also known as Supervisor Mode or Kernel Mode, is an execution environment in which the operating system kernel runs with unrestricted access to system resources. It can perform critical operations like accessing hardware directly, modifying system settings, and managing system memory.</p>\n    <p>The kernel operates in privileged mode to execute essential system tasks, such as process management, memory allocation, and hardware control, which require elevated privileges beyond what user mode programs can access.</p>\n\n    <h3>Protection Rings</h3>\n    <p><b>Protection rings</b> are a mechanism used by processors to enforce access control and privilege levels within the system. Modern processors typically support multiple protection rings, with <b>Ring 0</b> being the most privileged (kernel mode) and higher-numbered rings being less privileged (user mode).</p>\n    <img src=\"https://static.takeuforward.org/content/-AjgFbAN7\" alt=\"Protection Rings Diagram\", width=400>\n\n    <p>The operating system uses protection rings to define different privilege levels for executing code and accessing system resources. User mode programs operate within the least privileged protection ring, while the kernel operates within the most privileged ring, allowing the kernel to control access to system resources and ensure system security.</p>\n\n    <h3>Transition between Kernel and User Modes</h3>\n    <p>A computer system switches execution context from running in privileged mode (kernel mode) to running in restricted mode (user mode) to maintain security and stability while allowing user programs to access privileged resources through controlled means.</p>\n    <img src=\"https://static.takeuforward.org/content/-hFXkPEpz\" alt=\"Transition between Kernel and User Modes\", width=400>\n\n    <p>When a computer system boots up, it typically starts executing user programs in user mode. In user mode, programs have restricted access to system resources and cannot perform privileged operations directly.</p>\n    <p>When a user program needs to perform a privileged operation or access protected resources (such as reading from or writing to disk), it makes a request to the kernel through a mechanism known as a <b>system call</b>.</p>\n    <p>Upon receiving a system call request, the CPU switches execution context from user mode to kernel mode. This transition involves changing the processor's execution mode and switching to a separate stack (kernel stack) reserved for handling kernel-level operations.</p>\n    <p>In kernel mode, the operating system kernel gains full access to system resources and can perform privileged operations on behalf of the user program. It validates the system call request, checks for necessary permissions, and executes the requested operation.</p>\n    <p>After completing the requested operation, the kernel prepares the result (if any) and copies it back to the user space. It also updates the CPU registers to restore the user program's execution state.</p>\n    <p>Once the system call handling is complete, the CPU switches execution context back to user mode. The program resumes execution from the point where the system call was invoked, continuing its normal flow with the result of the system call operation.</p>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Process-and-its-states",
        "sl_no_in_step": 7,
        "title": "Process and its states",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/process-and-its-states",
        "content": " <p>A <b>Process</b> is an active task on a computer system that requires a set of resources, including memory, CPU time, and special registers to perform its functions. It is an instance of a program in execution.</p>\n    <p>A process can be considered as a unit of work that has been submitted by the user. It consists of an executable program along with the associated data and resources required for its execution.</p>\n\n    <img src=\"https://static.takeuforward.org/content/-WmSSyGKA\" alt=\"Process Management\", width=400>\n\n    <p><b>Process Management</b> involves the activities related to the creation, manipulation, scheduling, and termination of processes in an operating system. It includes functions such as process creation, process scheduling, process synchronisation, and inter-process communication.</p>\n    <p>The primary goal of process management is to allocate resources to processes in a fair and efficient manner to ensure optimal utilisation of system resources.</p>\n\n    <img src=\"https://static.takeuforward.org/content/-HzdqOYv2\" alt=\"Multithreading\", width=400>\n\n    <p><b>Multithreading</b> is the capability of an operating system to support multiple threads within a single process. A thread is a lightweight process that shares the same memory space and resources with other threads within the same process. Multithreading allows concurrent execution of multiple threads within a process, enabling better utilisation of CPU resources and enhanced responsiveness in applications.</p>\n    <p>Each thread within a process can perform independent tasks and share data and resources with other threads, facilitating efficient parallelism and concurrency in program execution.</p>\n\n    <h3>Process Control Block (PCB) and Thread Control Block (TCB)</h3>\n    <p>The <b>Process Control Block (PCB)</b> and <b>Thread Control Block (TCB)</b> are data structures used by operating systems to manage processes and threads, respectively. They contain essential information about the state, resources, and execution context of processes and threads, allowing the operating system to effectively manage and control their execution.</p>\n\n    <img src=\"https://static.takeuforward.org/content/-hKbrsgkN\" alt=\"Process and Thread Control Blocks\">\n\n    <h4>Process Control Block (PCB)</h4>\n    <p>PCBs are created by the operating system when a process starts and are updated throughout its lifetime. They are assigned a unique identifier for distinguishing them from other processes and contain status information indicating if the process is on hold, ready, running, or waiting.</p>\n    <p>It stores process state, including the instruction counter and register contents when not running, and holds the CPU register contents if interrupted. PCB includes details of memory allocation, such as address and virtual memory mapping, and lists all allocated resources like hardware units or files. They are used by priority scheduling algorithms for determining execution order and hold accounting information for billing and performance measurement.</p>\n\n    <h4>Thread Control Block (TCB)</h4>\n    <p>TCBs are created by the operating system when a thread is initiated and are updated during its execution. Each TCB is assigned a unique identifier to distinguish it from other threads. It contains information about the thread's current state, indicating if it's ready, running, or waiting.</p>\n    <p>The TCB stores the execution context, including details about the current instruction being executed and the data being used. Additionally, it includes scheduling information used by the scheduler to manage thread execution, such as priority levels and scheduling parameters.</p>\n\n    <h3>Process States</h3>\n    <img src=\"https://static.takeuforward.org/content/-u_O7JzCA\" alt=\"Process States\">\n    <p>A process undergoes various states through the Operating System, from being initially placed on hold to its eventual completion. In a multiprogramming system, where the CPU can be allocated to many jobs, each with numerous processes, managing processor resources becomes quite complex.</p>\n    <p>Initially, when a user submits a job into the system, it enters the hold state. This state is controlled by the Job Scheduler, which places the job in a queue for further processing. At this stage, characteristics of each job, such as CPU time estimate, priority, and resource requirements, are noted down in a table by the job spooler or disk controller.</p>\n    <p>Once the interrupts related to the job are resolved, it transitions to the ready state. The running state signifies that the job is actively being processed by the CPU. In a single-processor system, only one job or process can be in the running state at any given time.</p>\n    <p>If a job encounters a situation where it cannot proceed, such as waiting for a specific resource allocation or an I/O operation to finish, it enters the waiting state. Upon resolving the waiting condition, the job transitions back to the ready state, indicating its readiness to resume execution.</p>\n    <p>Finally, when a job completes its execution, it enters the finished state and is returned to the user. Throughout these transitions, the Job Scheduler and Process Scheduler play crucial roles. The Job Scheduler initiates transitions between hold, ready, and finished states based on predefined policies, while the Process Scheduler manages transitions between ready, running, and waiting states based on predefined algorithms.</p>\n\n    <h3>Process Creation and Termination</h3>\n    <p>When a process is created, the operating system allocates the necessary resources, such as memory space, registers, and system data structures like the Process Control Block (PCB). This involves initialising the process state, assigning a unique identifier, and setting up the execution environment. The creation process typically involves invoking system calls or library functions, which may include specifying the executable program, defining command-line arguments, and configuring process attributes.</p>\n\n    <img src=\"https://static.takeuforward.org/content/-kiVuyM2Y\" alt=\"Process Creation and Termination\">\n\n    <p>On the other hand, process termination involves releasing all resources associated with the process and reclaiming memory and system structures. This includes deallocating memory space, closing open files, releasing hardware resources, and updating relevant data structures like process tables. Additionally, the operating system may perform cleanup tasks, such as notifying parent processes, updating accounting information, and handling any child processes. Proper process termination is essential for maintaining system stability and resource efficiency, ensuring that system resources are efficiently managed and available for allocation to other processes.</p>\n\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>\n\n"
      },
      {
        "id": "Normal-function-call-vs-System-Calls-in-OS",
        "sl_no_in_step": 8,
        "title": "Normal function call vs System Calls in OS",
        "yt_link": "https://youtu.be/gEGMphclBvY?si=oPG99zUNQu0F_9E6",
        "article_link": "https://takeuforward.org/operating-system/normal-function-call-system-call-in-os",
        "content": "<h3>Function Calls</h3>\n    <p>Function calls are used to execute a specific block of code like a function or subroutine within a program. They run <b>within the same execution context</b> or address space as the calling program.</p>\n    <p>Function calls are suitable for tasks that are part of the program itself, such as:</p>\n    <ul>\n        <li>Performing calculations</li>\n        <li>Manipulating data structures</li>\n        <li>Invoking utility functions</li>\n    </ul>\n    <p><b>Parameters</b> and data are passed to functions through the program's stack or registers, depending on the programming language and system architecture.</p>\n    <p>Function calls are usually resolved <b>at compile-time or link-time</b>, which means they have a lower execution overhead compared to system calls.</p>\n\n    <h3>System Calls</h3>\n    <p>System calls provide a way for applications to <b>request services from the operating system</b>. Unlike function calls, system calls run in a different execution context or address space, as they need to interact with the privileged kernel mode of the OS.</p>\n    <img src=\"https://static.takeuforward.org/content/-WmSSyGKA\" alt=\"System Call Illustration\", width=400>\n    <p>System calls are used to access operating system features like:</p>\n    <ul>\n        <li>File I/O (e.g., reading or writing files)</li>\n        <li>Process management (e.g., creating or terminating processes)</li>\n        <li>Memory allocation</li>\n        <li>Inter-process communication</li>\n    </ul>\n    <p>They abstract the details of the OS implementation, providing a standard way to access its services.</p>\n    <p><b>Parameters</b> for system calls are passed in a different way compared to function calls, as they have to transition between <b>user space and kernel space</b>. This can involve passing parameters in registers, memory blocks, or the stack.</p>\n    <img src=\"https://static.takeuforward.org/content/-WmSSyGKA\" alt=\"System Call Process\", width=400>\n    <p>System calls are typically implemented as software interrupts or traps, which <b>transfer control to the kernel</b>. This transition causes system calls to have higher execution overhead compared to function calls.</p>\n\n    <h3>Difference between Function Calls and System Calls</h3>\n    <table>\n        <tr>\n            <th>Aspect</th>\n            <th>Function Calls</th>\n            <th>System Calls</th>\n        </tr>\n        <tr>\n            <td><b>Execution Context</b></td>\n            <td>Within the same execution context/address space as the calling program</td>\n            <td>Operate in a different execution context/address space, interacting with the OS kernel</td>\n        </tr>\n        <tr>\n            <td><b>Purpose</b></td>\n            <td>Execute specific blocks of code within the program</td>\n            <td>Request services from the operating system</td>\n        </tr>\n        <tr>\n            <td><b>Usage</b></td>\n            <td>Used for tasks like calculations, data manipulation, and utility functions</td>\n            <td>Used for accessing OS functionalities like file I/O, process management</td>\n        </tr>\n        <tr>\n            <td><b>Parameters Passing</b></td>\n            <td>Passed via the program's stack or registers</td>\n            <td>Passed differently, often bridging user space and kernel space</td>\n        </tr>\n        <tr>\n            <td><b>Resolution Time</b></td>\n            <td>Resolved at compile-time or link-time</td>\n            <td>Resolved at runtime when invoked by the program</td>\n        </tr>\n        <tr>\n            <td><b>Overhead</b></td>\n            <td>Lower execution overhead</td>\n            <td>Higher execution overhead due to context switching and kernel interaction</td>\n        </tr>\n        <tr>\n            <td><b>Example</b></td>\n            <td><code>int sum = add(2, 3);</code></td>\n            <td><code>int fd = open(\"file.txt\", O_RDONLY);</code></td>\n        </tr>\n    </table>\n\n    <h3>Example Application: Calculator</h3>\n    <p><b>Function Calls:</b> In a calculator application, function calls like <code>add</code>, <code>subtract</code>, <code>multiply</code>, and <code>divide</code> are used to perform arithmetic operations. These functions are called whenever the user performs an operation like adding two numbers.</p>\n    <p><b>System Calls:</b> If the calculator needs to <b>store the calculation history</b> in a file, it would use system calls. For example, it could use system calls to write the results to a text file or read the stored history.</p>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>\n"
      }
    ]
  },
  {
    "step_no": 2,
    "topic": "Process Management",
    "data": [
      {
        "id": "Process-scheduling-algorithms",
        "sl_no_in_step": 1,
        "title": "Process scheduling algorithms(Preemptive and Non-Preemptive)",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/process-scheduling-preemptive-and-non-preemptive",
        "content": "<p>Process Scheduling is a key aspect of operating system design. It handles the <b>efficient allocation of system resources</b> like CPU time and memory to multiple processes or threads running at the same time. It ensures processes are executed in a way that <b>optimises system performance</b>, resource utilisation, and user responsiveness.</p>\n    <img src=\"https://static.takeuforward.org/content/-2uv5Wj6k\" alt=\"Process Scheduling Illustration\", width=400>\n    <p>This mechanism allows the operating system to select a process from the <b>ready queue</b> and give it CPU time for execution. It decides which process should run next, how long it should run, and when to preempt or suspend it to let other processes execute.</p>\n\n    <h3>Pre-emptive and Non-Preemptive Scheduling</h3>\n\n    <h3>Preemptive Scheduling</h3>\n    <p>In <b>Preemptive Scheduling</b>, the operating system can <b>interrupt a currently running process</b> and give the CPU to another process. This can happen if a higher-priority process arrives, a timer indicates the current process's time slice is over, or an event needing immediate attention occurs.</p>\n    <p>In preemptive scheduling, processes may be <b>suspended or preempted</b> even if they haven't voluntarily given up the CPU. The operating system has more control over process execution, which allows it to allocate CPU resources dynamically.</p>\n    <p>This type of scheduling is ideal for situations where <b>responsiveness and fairness</b> are crucial, such as in real-time systems and multitasking environments.</p>\n    <img src=\"https://static.takeuforward.org/content/-B7zj9_ct\" alt=\"Preemptive Scheduling Illustration\", width=400>\n\n    <h3>Non-Preemptive Scheduling</h3>\n    <p>In <b>Non-Preemptive Scheduling</b>, a running process keeps the CPU until it voluntarily releases it by completing its task, waiting for an I/O operation, or entering a waiting state.</p>\n    <p>Here, processes are <b>not interrupted</b> while executing. They keep control of the CPU until they finish or willingly yield it. This type of scheduling is easier to implement and usually has <b>lower overhead</b> compared to preemptive scheduling, but it might lead to <b>slower responsiveness</b> if long-running processes dominate CPU time.</p>\n    <img src=\"https://static.takeuforward.org/content/-C-u366Wf\" alt=\"Non-Preemptive Scheduling Illustration\">\n\n    <h3>Differences between Pre-emptive and Non-Preemptive Scheduling</h3>\n    <table>\n        <tr>\n            <th>Aspect</th>\n            <th>Preemptive</th>\n            <th>Non-Preemptive</th>\n        </tr>\n        <tr>\n            <td><b>Interruption</b></td>\n            <td>Processes can be interrupted while executing</td>\n            <td>Processes cannot be interrupted while executing</td>\n        </tr>\n        <tr>\n            <td><b>Control of CPU</b></td>\n            <td>Operating system controls CPU allocation</td>\n            <td>Processes retain control of the CPU until completion</td>\n        </tr>\n        <tr>\n            <td><b>Response Time</b></td>\n            <td>Typically shorter response times</td>\n            <td>Response times may be longer</td>\n        </tr>\n        <tr>\n            <td><b>Fairness</b></td>\n            <td>Allows for fairness by using priority-based scheduling</td>\n            <td>May face fairness issues if long-running processes dominate the CPU</td>\n        </tr>\n        <tr>\n            <td><b>Complexity</b></td>\n            <td>More complex due to dynamic process switching</td>\n            <td>Simpler to implement and manage</td>\n        </tr>\n        <tr>\n            <td><b>Resource Utilisation</b></td>\n            <td>Generally more efficient CPU utilisation</td>\n            <td>May be less efficient if processes monopolise CPU</td>\n        </tr>\n        <tr>\n            <td><b>Suitable Environments</b></td>\n            <td>Ideal for multitasking and real-time systems</td>\n            <td>Suitable for simpler systems or those needing predictable behavior</td>\n        </tr>\n    </table>\n\n    <h3>Real-world Examples</h3>\n    <h3>Preemptive Scheduling</h3>\n    <ul>\n        <li>Preemptive scheduling is crucial for modern multitasking operating systems like Windows, macOS, and Linux. These systems run multiple processes at the same time and ensure each process gets fair CPU time, allowing high-priority tasks to be executed promptly.</li>\n        <li>In <b>real-time systems</b>, such as automotive systems, preemptive scheduling ensures timely responses to sensor inputs or critical actions like applying brakes, which requires quick reactions.</li>\n        <li>For <b>network servers</b>, preemptive scheduling is beneficial for handling multiple client requests simultaneously. It ensures that no single client request dominates CPU resources, keeping the system responsive for all clients.</li>\n    </ul>\n\n    <h3>Non-Preemptive Scheduling</h3>\n    <ul>\n        <li>Non-preemptive scheduling is used in <b>resource-constrained embedded systems</b> where timing requirements are strict. It simplifies timing analysis and ensures predictable execution, which is ideal for systems like industrial automation or IoT devices.</li>\n        <li>It is also suitable for <b>batch processing systems</b> where tasks run sequentially without interruption. For example, in payroll systems, each calculation task can complete uninterrupted, ensuring data consistency.</li>\n        <li>Non-preemptive scheduling can be used in <b>single-user systems</b> where users expect consistent behavior rather than responsiveness, such as in personal computers running desktop applications.</li>\n    </ul>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Scheduling-algorithms",
        "sl_no_in_step": 2,
        "title": "Scheduling algorithms(FCFS, SJF, SRTF, HRNN, RR, PBS, MLQ, MLFQ)",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/scheduling-algorithms",
        "content": "<h3>First-Come, First-Served (FCFS)</h3>\n    <p>In <b>FCFS scheduling</b>, processes are executed in the order they arrive in the ready queue. The CPU is allocated to the first process in the queue, and it continues execution until it completes or enters a waiting state.</p>\n    <img src=\"https://static.takeuforward.org/content/-MH8adAmn\" alt=\"FCFS Scheduling Illustration\" width=\"400\">\n    <ul>\n        <li>It is simple and easy to implement.</li>\n        <li>Fairness issues can arise if long-running processes take over CPU time.</li>\n        <li>Can lead to poor average waiting times, especially for long processes that arrive early.</li>\n    </ul>\n\n    <h3>Shortest Job First (SJF)</h3>\n    <p><b>SJF scheduling</b> selects the process with the shortest estimated CPU burst time next. This algorithm minimises the average waiting time by prioritising short processes.</p>\n    <img src=\"https://static.takeuforward.org/content/-r0pleNsq\" alt=\"SJF Scheduling Illustration\" width=\"400\">\n    <ul>\n        <li>Requires knowledge of the CPU burst times of processes (either estimated or actual).</li>\n        <li>Optimal for minimising average waiting time but can lead to starvation for long processes if short processes frequently arrive.</li>\n    </ul>\n\n    <h3>Shortest Remaining Time First (SRTF)</h3>\n    <p><b>SRTF</b> is a preemptive version of SJF where the currently executing process can be preempted by a shorter job. Whenever a new process arrives, the scheduler compares its burst time with the remaining time of the currently executing process and switches if the new process has a shorter remaining time.</p>\n    <img src=\"https://static.takeuforward.org/content/-5X9Ljlud\" alt=\"SRTF Scheduling Illustration\" width=\"400\">\n    <ul>\n        <li>SRTF provides optimal average waiting time.</li>\n        <li>May cause frequent context switches, leading to increased overhead.</li>\n    </ul>\n\n    <h3>Round-Robin (RR)</h3>\n    <p><b>RR scheduling</b> allocates CPU time to each process in turn, with a fixed time quantum (also known as time slice). When a time quantum expires, the currently executing process is preempted, and the CPU is allocated to the next process in the queue.</p>\n    <img src=\"https://static.takeuforward.org/content/-dLFdHae5\" alt=\"RR Scheduling Illustration\" width=\"400\">\n    <ul>\n        <li>Prevents starvation by ensuring that every process gets a fair share of CPU time.</li>\n        <li>Fairly simple to implement.</li>\n        <li>Performance depends on the choice of time quantum; shorter quantum improves response time but increases overhead.</li>\n    </ul>\n\n    <h3>Multilevel Queue</h3>\n    <p><b>Multilevel queue scheduling</b> divides the ready queue into multiple queues, each with its own scheduling algorithm and priority level. Processes are assigned to different queues based on criteria such as process type, priority, or characteristics.</p>\n    <img src=\"https://static.takeuforward.org/content/-wsz3uWRR\" alt=\"Multilevel Queue Scheduling Illustration\" width=\"400\">\n    <ul>\n        <li>Allows for the classification of processes into different categories and the application of different scheduling policies.</li>\n        <li>Processes may migrate between queues based on their behaviour or priority level.</li>\n    </ul>\n\n    <h3>Multilevel Queue with Feedback</h3>\n    <p><b>Multilevel queue with feedback scheduling</b> is an extension of multilevel queue scheduling where processes can move between queues based on their CPU burst time or execution behaviour. If a process uses up its time quantum without completing, it is moved to a lower-priority queue to prevent starvation.</p>\n    <img src=\"https://static.takeuforward.org/content/-r40OMD2_\" alt=\"Multilevel Queue with Feedback Scheduling Illustration\" width=\"400\">\n    <ul>\n        <li>Provides dynamic priority adjustment based on process behaviour.</li>\n        <li>Prevents starvation of short processes in higher-priority queues.</li>\n        <li>More complex to implement compared to a simple multilevel queue.</li>\n    </ul>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>\n"
      },
      {
        "id": "Starving-and-Aging",
        "sl_no_in_step": 3,
        "title": "Starving and Aging",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/starving-aging",
        "content": "<h3>Starvation in Process Scheduling</h3>\n    <p><b>Starvation</b> in process scheduling happens when a process is repeatedly denied the resources it needs because other processes are prioritized. This often occurs in priority-based scheduling systems, where lower-priority processes may be delayed indefinitely if higher-priority ones keep arriving.</p>\n    <p>Starvation can be a serious issue because it may lead to <b>inefficient system performance</b>, causing some processes to never execute, which can make the system unstable and leave certain tasks unfinished.</p>\n    <img src=\"https://static.takeuforward.org/content/-UXsjBMYb\" alt=\"Starvation in Process Scheduling\" >\n\n    <h3>Ageing as a Solution to the Starvation Problem</h3>\n    <p><b>Ageing</b> is a method to prevent starvation by gradually increasing a process's priority as it waits in the queue. This ensures that all processes eventually become high-priority if they wait long enough, allowing them to get their turn for execution.</p>\n    <p>Ageing helps maintain balance in the system, letting <b>low-priority processes</b> move up in priority over time. This prevents long waits and ensures a fairer distribution of resources for all processes.</p>\n    <img src=\"https://static.takeuforward.org/content/-XkUyRC7-\" alt=\"Ageing in Process Scheduling\" >\n\n    <h3>Starvation and Ageing in Different Scheduling Algorithms</h3>\n    <ul>\n        <li><b>First-Come, First-Served (FCFS)</b>: Generally, FCFS avoids starvation because processes are handled in the order they arrive. But it can have a <b>convoy effect</b>, where short processes are delayed behind long ones.</li>\n        <li><b>Shortest Job Next (SJN)</b>: This can lead to starvation for longer processes, as shorter ones are always prioritized. <b>Ageing</b> can be used to adjust the priority of longer processes over time.</li>\n        <li><b>Priority Scheduling</b>: Prone to starvation for lower-priority processes. Ageing is often applied here to gradually increase the priority of waiting processes.</li>\n        <li><b>Round Robin (RR)</b>: RR usually prevents starvation since every process gets a fixed time slice in a cycle. Ageing is less needed because all processes get CPU time.</li>\n    </ul>\n\n    <h3>Real-world Examples</h3>\n    <p>In a multi-user operating system, imagine a scenario where a <b>background system maintenance process</b> has lower priority than user applications. If user applications keep arriving, the maintenance process might starve and never get CPU time.</p>\n    <p>By using ageing, the maintenance process’s priority would gradually increase, ensuring it eventually gets CPU time even with many user applications. Another example is in <b>network packet scheduling</b>, where low-priority packets might starve if high-priority ones keep coming. Ageing helps ensure all packets are eventually transmitted.</p>\n\n    <h3>Preventing Starvation in Scheduling</h3>\n    <p>To prevent starvation, one can implement <b>ageing</b> in priority-based systems, so all processes gain priority over time and eventually get executed. Another approach is to use <b>Round Robin scheduling</b>, which naturally avoids starvation by giving each process a turn.</p>\n    <p>Additionally, setting a <b>maximum wait time</b> or using <b>feedback queues</b> can help. In feedback queues, processes move between different priority levels based on their waiting time or service needs. These methods help ensure a balanced and fair distribution of resources across all processes.</p>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Producer-consumer-problem",
        "sl_no_in_step": 4,
        "title": "Producer-consumer problem",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/producer-consumer",
        "content": "    <p>The <b>producer-consumer synchronisation</b> is used to manage the communication between two software processors: producers and consumers.</p>\n    <ul>\n        <li><b>Producers</b> are responsible for generating data or items that need to be consumed by other parts of the system. Producers add items to a shared buffer or queue.</li>\n        <li><b>Consumers</b> are responsible for consuming or processing the items generated by the producers. Consumers retrieve items from the shared buffer or queue and process them.</li>\n    </ul>\n    <img src=\"https://static.takeuforward.org/content/-rqxpI42d\" alt=\"Producer-Consumer Synchronisation\" >\n\n    <h3>Shared Buffer/Queue</h3>\n    <p>There is a <b>Shared Buffer/Queue</b> that serves as the communication medium between producers and consumers. Producers add items to this buffer, and consumers remove items from it.</p>\n    <p>Both producers and consumers may run concurrently, meaning they can execute simultaneously. However, to prevent issues like race conditions or data corruption, proper synchronisation mechanisms must be used.</p>\n    <p><b>Producers</b> generate items and add them to the shared buffer. If the buffer is full, they may need to wait until space becomes available. <b>Consumers</b> retrieve and process items from the buffer, and may need to wait if it is empty.</p>\n    <p>Common synchronisation mechanisms include <b>locks, semaphores, condition variables</b>, or specialised data structures like blocking queues.</p>\n    <img src=\"https://static.takeuforward.org/content/-8jMwkKNn\" alt=\"Shared Buffer Concept\" width=\"400\">\n\n    <h3>Challenges in Producer-Consumer Processing</h3>\n    <ul>\n        <li><b>Data Synchronisation</b>: Ensures that producers and consumers operate in a coordinated way, avoiding issues like race conditions, data corruption, or buffer overflow/underflow. It also ensures that the order of produced and consumed items is maintained.</li>\n        <li><b>Mutual Exclusion</b>: Enforces that only one producer or consumer can access the shared buffer at a time, preventing conflicts and maintaining data integrity. It is also important to avoid deadlocks, where producers or consumers wait indefinitely for resources held by each other.</li>\n        <li><b>Resource Utilisation</b>: It is crucial to manage the buffer size to avoid <b>buffer overflow</b> (when full) or <b>buffer underflow</b> (when empty). The rates of production and consumption must be balanced to prevent inefficiency or system instability.</li>\n    </ul>\n\n    <h3>Solutions to the Producer-Consumer Problem</h3>\n    <h3>Semaphores</h3>\n    <p>A <b>semaphore</b> is a synchronisation primitive used in concurrent programming to control access to a shared resource by multiple processes or threads.</p>\n    <p>Two semaphores can be used to represent the empty and full slots in the buffer. Producers decrement the empty semaphore when adding items and increment the full semaphore when adding items. Consumers do the reverse.</p>\n    <img src=\"https://static.takeuforward.org/content/-jZFHKnNa\" alt=\"Semaphore in Producer-Consumer\" >\n\n    <h3>Using Monitors</h3>\n    <p><b>Monitors</b> encapsulate shared data and procedures. Condition variables are used to block and wake up threads waiting for certain conditions. Producers wait on a condition variable when the buffer is full, and consumers wait when the buffer is empty. Signal operations notify waiting threads when the buffer state changes.</p>\n    <img src=\"https://static.takeuforward.org/content/-Qjk7TldJ\" alt=\"Monitors in Producer-Consumer\" width=\"400\">\n\n    <h3>Mutex and Condition Variables</h3>\n    <p>A <b>mutex</b> (lock) ensures mutual exclusion when accessing the shared buffer. Producers and consumers acquire and release this mutex to prevent concurrent access.</p>\n    <p><b>Condition variables</b> can block and unblock producers and consumers based on the buffer's state. Producers wait if the buffer is full, and consumers wait if the buffer is empty.</p>\n    <img src=\"https://static.takeuforward.org/content/-deIRlbzj\" alt=\"Mutex and Condition Variables\" >\n\n    <h3>Blocking Queues</h3>\n    <p>Using a <b>blocking queue</b> simplifies the producer-consumer problem as it automatically handles synchronisation. Producers can push items into the queue, and consumers can pop items from it. If the queue is full, producers block until space is available; if the queue is empty, consumers block until items are available.</p>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Critical-Section-Problem",
        "sl_no_in_step": 5,
        "title": "Critical Section Problem",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/critical-section-problem",
        "content": "    <p>In concurrent programming, a <b>critical section</b> is a portion of code that accesses shared resources (such as variables, data structures, or files) which must be accessed by multiple threads or processes. It is critical because if multiple threads or processes execute this section concurrently, it could lead to unexpected behaviour, data corruption, or incorrect results due to race conditions.</p>\n    \n    <h3>Understanding the Critical Section Problem</h3>\n<img src=\"https://static.takeuforward.org/content/-2s3WCzOq\">\n    <p>The critical section problem arises when multiple processes or threads need to access and modify shared resources simultaneously. The main challenge is to ensure that only one process or thread enters the critical section at a time to prevent conflicts.</p>\n    <p>If not managed properly, this can lead to <b>race conditions</b>, where the system's behaviour depends on the sequence or timing of uncontrollable events, causing unpredictable and erroneous results known as data inconsistency.</p>\n    <p>For instance, if two threads simultaneously update a shared variable without proper synchronisation, the final value of the variable might be incorrect, leading to potential system failures or incorrect computations.</p>\n\n    <h3>Race Conditions</h3>\n    <p>A <b>race condition</b> occurs when the outcome of a program depends on the sequence or timing of uncontrollable events, such as the order in which threads are scheduled to run. This can lead to unpredictable and erroneous behaviour in concurrent systems.</p>\n    <p>Consider two threads attempting to increment a shared counter variable. If both read the counter's value simultaneously and then increment it, the final value will reflect only one increment instead of two, leading to incorrect results.</p>\n    <p>Race conditions often result in data inconsistency. For example, in a banking application, if two transactions are processed concurrently without proper synchronisation, the final account balance may be incorrect.</p>\n    <p>To prevent race conditions, synchronisation mechanisms like locks, mutexes, or other atomic operations are necessary. These ensure that only one thread accesses the critical section at a time.</p>\n\n    <h3>Deadlock</h3>\n    <img src=\"https://static.takeuforward.org/content/-5R_d3gl4\" alt=\"Deadlock Illustration\">\n    <p>A <b>deadlock</b> is a situation where a set of processes are blocked because each process is holding a resource and waiting for another resource held by another process. The conditions for a deadlock to occur include:</p>\n    <ul>\n        <li><b>Mutual Exclusion:</b> At least one resource must be held in a non-shareable mode.</li>\n        <li><b>Hold and Wait:</b> A process must be holding at least one resource while waiting to acquire additional resources.</li>\n        <li><b>No Preemption:</b> Resources cannot be forcibly taken from processes until they voluntarily release them.</li>\n        <li><b>Circular Wait:</b> A set of processes exists where each process waits for a resource held by the next process in the sequence.</li>\n    </ul>\n    <p>To prevent deadlock, multiple strategies can be employed, such as algorithms like the Banker's algorithm, which dynamically examine resource allocation states to ensure that circular wait conditions do not occur. Techniques to detect deadlock and methods to recover from it include resource preemption and process termination.</p>\n\n    <h3>Starvation</h3>\n    <p><b>Starvation</b> is a situation where a process is perpetually denied necessary resources to proceed with its execution. It is often caused by priority scheduling, where low-priority processes never get the resources they need if high-priority processes keep preempting them.</p>\n    <p>To prevent starvation, techniques such as <b>aging</b> can be used, where the priority of a process increases the longer it waits.</p>\n\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>\n"
      },
      {
        "id": "Process-synchronization-and-its-tools",
        "sl_no_in_step": 6,
        "title": "Process synchronization and its tools",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/process-synchronisation-and-its-tools",
        "content": "<p>Process Synchronisation refers to the coordination of processes to ensure that they can operate concurrently without interfering with each other, particularly when they need to access shared resources. In concurrent systems, where multiple processes or threads execute simultaneously, synchronisation is critical to maintain consistency and correctness of data, prevent race conditions, and avoid other issues such as deadlocks and starvation.</p>\n\n    <h2>Process Synchronisation in Concurrent Systems</h2>\n\n    <h3>Maintaining Data Consistency</h3>\n    <p>When multiple processes or threads access and modify shared resources (like variables, files, or databases), there is a risk of data inconsistency. Synchronisation mechanisms ensure that only one process accesses the critical section of code that modifies shared resources at any given time. In a banking system, if two processes simultaneously update an account balance without synchronisation, the final balance might reflect only one of the updates, leading to incorrect results.</p>\n\n    <h3>Preventing Race Conditions</h3>\n    <p>Race Conditions occur when the system's behaviour depends on the sequence or timing of uncontrollable events, leading to unpredictable results. Proper synchronisation ensures that the operations in the critical section are executed in a controlled manner. Two threads incrementing a shared counter without synchronisation might both read the same initial value and write back the same incremented value, resulting in an incorrect final count.</p>\n\n<img src=\"https://static.takeuforward.org/content/-5R_d3gl4\", width=400>\n    <h3>Avoiding Deadlocks</h3>\n    <p>Deadlocks happen when a set of processes are blocked because each process is waiting for a resource held by another process. Synchronisation techniques, such as lock hierarchies and timeout mechanisms, help prevent deadlocks by ensuring proper resource allocation and release. If two processes each hold a lock and try to acquire the lock held by the other, they will be stuck waiting indefinitely unless proper deadlock avoidance strategies are in place.</p>\n\n    <h3>Fair Resource Allocation</h3>\n    <p>Starvation occurs when a process is perpetually denied access to the resources it needs for execution. Synchronisation mechanisms can include fairness policies to ensure that all processes get a chance to execute.</p>\n\n    <h2>Synchronisation Mechanisms</h2>\n    <h3>Locks and Mutexes</h3>\n    <p>Provide mutual exclusion, ensuring that only one thread or process can enter the critical section at a time.</p>\n\n<img src=\"https://static.takeuforward.org/content/-deIRlbzj\">\n\n    <h3>Semaphores</h3>\n    <p>Use signalling mechanisms to control access to resources, allowing multiple threads to use resources concurrently while avoiding conflicts.</p>\n\n<img src=\"https://static.takeuforward.org/content/-jZFHKnNa\">\n\n    <h3>Monitors</h3>\n    <p>Combine mutual exclusion locks with condition variables to manage access to shared resources in a structured way.</p>\n\n    <h3>Atomic Operations</h3>\n    <p>Provide a way to perform operations atomically without the need for explicit locks, reducing overhead.</p>\n\n    <h3>Condition Variables</h3>\n    <p>Condition variables are used in conjunction with mutexes to allow threads to wait for certain conditions to become true. A thread can wait on a condition variable and will be awakened when another thread signals that the condition has changed. This mechanism is useful for scenarios where a thread needs to wait until a specific resource is available or a particular state is reached.</p>\n\n    <h3>Read-Write Locks</h3>\n    <p>Read-write locks allow concurrent read access for multiple threads while providing exclusive write access for a single thread. This mechanism improves performance in scenarios where read operations are more frequent than write operations, as multiple threads can read the data simultaneously without waiting for write locks.</p>\n\n<img src=\"https://static.takeuforward.org/content/-4-3QCuy_\">\n\n    <h3>Barrier Synchronisation</h3>\n    <p>Barrier synchronisation allows multiple threads to wait until all threads reach a certain point in execution before proceeding. This is useful in parallel programming, where it is necessary to ensure that all threads have completed their current task before moving on to the next stage, thus coordinating their progress in a structured manner.</p>\n"
      },
      {
        "id": "Semaphores-and-its-type",
        "sl_no_in_step": 7,
        "title": "Semaphores and its type",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/semaphore-and-its-types",
        "content": "    <p><b>Semaphores</b> are synchronisation primitives used to manage concurrent processes by controlling access to shared resources in a computing environment. They are essential for preventing <b>race conditions</b>, where multiple processes attempt to access and modify shared data simultaneously, leading to unpredictable outcomes.</p>\n    <img src=\"https://static.takeuforward.org/content/-yt3fwRHm\" alt=\"Semaphores Concept\">\n    <p>A semaphore is essentially a variable that signals whether a particular resource is available. Using operations like <b>wait</b> (or P) and <b>signal</b> (or V), semaphores coordinate the activities of different processes, ensuring that only a specified number of processes can access the critical section at any given time. This coordination is crucial in multi-threaded applications to maintain data consistency and integrity.</p>\n\n    <h3>Binary Semaphores and Counting Semaphores</h3>\n    <h4>Binary Semaphores</h4>\n    <p>Binary semaphores, also known as <b>mutexes</b> (mutual exclusion), have only two states: 0 and 1. They ensure that a resource is accessed by only one thread at a time. When a thread acquires the semaphore, it sets the value to 0, and when it releases the semaphore, it sets the value back to 1.</p>\n    <img src=\"https://static.takeuforward.org/content/-7IT6Za_R\" alt=\"Binary Semaphore\">\n    <p>Binary semaphores are typically used for simple lock/unlock mechanisms where mutual exclusion is required.</p>\n\n    <h4>Counting Semaphores</h4>\n    <p>Counting semaphores can take on any non-negative integer value and are used to control access to a resource with a limited number of instances. The semaphore's value is initialised to the number of available resources. Each <b>wait</b> operation decrements the value, and each <b>signal</b> operation increments it. If the value is zero, indicating no resources are available, the process attempting to decrement the semaphore is blocked until another process increments it.</p>\n    <img src=\"https://static.takeuforward.org/content/-4-3QCuy_\" alt=\"Counting Semaphore\">\n    <p>This type of semaphore is useful in scenarios where a pool of identical resources needs to be managed, such as a fixed number of database connections.</p>\n\n    <h3>Semaphores Vs Other Synchronisation Primitives</h3>\n    <p>Semaphores are often compared with other synchronisation primitives like <b>locks, monitors,</b> and <b>condition variables</b>:</p>\n    <ul>\n        <li>Unlike locks, which are used for mutual exclusion, semaphores offer more versatility with their counting capability, allowing multiple instances of a resource to be managed.</li>\n        <li>Monitors encapsulate both mutual exclusion and the condition variables needed for complex waiting scenarios, offering a higher level of abstraction compared to semaphores.</li>\n        <li>Condition variables, used with locks, allow threads to wait for certain conditions to be met, sometimes making them easier to use than semaphores.</li>\n    </ul>\n    <p>However, semaphores provide a simpler and more direct way to implement certain synchronisation scenarios, especially those involving limited resource pools.</p>\n\n    <h3>Deadlock Avoidance</h3>\n    <p><b>Deadlocks</b> occur when two or more processes wait indefinitely for resources held by each other. To avoid deadlocks with semaphores, techniques such as resource ordering, timeout mechanisms, and deadlock detection algorithms can be employed. Ensuring that all semaphores are requested in a predefined global order can prevent circular wait conditions, a key factor in deadlock.</p>\n    <img src=\"https://static.takeuforward.org/content/-5R_d3gl4\" alt=\"Deadlock Avoidance\" width=\"400\">\n\n    <h3>Priority Inversion</h3>\n    <p><b>Priority inversion</b> happens when a higher-priority task is waiting for a resource held by a lower-priority task, causing the higher-priority task to be indirectly preempted. This can be mitigated using <b>priority inheritance protocols</b>, where a lower-priority task temporarily inherits the higher priority of the blocked task, ensuring that critical sections are exited more quickly.</p>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Deadlock-and-Methods-for-preventing-deadlock",
        "sl_no_in_step": 8,
        "title": "Deadlock and Methods for preventing deadlock",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/deadlock-in-concurrent-systems",
        "content": "<p>Deadlock occurs in concurrent systems when a set of processes are each waiting for an event that only another process in the set can cause. As a result, none of the processes can proceed, and the system is essentially stuck.</p>\n    <img src=\"https://static.takeuforward.org/content/-5R_d3gl4\" alt=\"Deadlock Representation\", width=400>\n\n    <h3>Key Characteristics of Deadlock</h3>\n    <p>Key characteristics of deadlock include:</p>\n    <ul>\n        <li>Processes being in a state of indefinite wait.</li>\n        <li>No single process being able to progress due to unavailable resources held by other waiting processes.</li>\n    </ul>\n    <p>Deadlock is often represented by a resource allocation graph where a cycle indicates its presence. Understanding these characteristics is essential for diagnosing and addressing deadlock in multi-threaded or multi-process environments.</p>\n\n    <h3>Necessary Conditions for Deadlock to Occur</h3>\n    <ul>\n        <li><b>Mutual Exclusion:</b> At least one resource must be held in a non-shareable mode, meaning only one process can use it at any given time.</li>\n        <li><b>Hold and Wait:</b> A process must hold at least one resource while waiting to acquire additional resources currently held by others.</li>\n        <li><b>No Preemption:</b> Resources cannot be forcibly taken from processes until they voluntarily release them.</li>\n        <li><b>Circular Wait:</b> A set of processes exists where each process waits for a resource held by the next process in the sequence, forming a loop.</li>\n    </ul>\n\n    <h3>Detecting and Handling Deadlocks</h3>\n    <p>Various methods exist for detecting and handling deadlock, each with its strengths and trade-offs:</p>\n    <ul>\n        <li><b>Deadlock Prevention:</b> This method designs systems so that one of the necessary conditions for deadlock cannot occur.</li>\n        <li><b>Deadlock Avoidance:</b> This method dynamically examines resource allocation to ensure the system never enters a deadlock state, using algorithms like the Banker's algorithm.</li>\n        <li><b>Deadlock Detection:</b> This allows the system to enter a deadlock state but periodically checks for deadlock conditions using techniques like resource allocation graphs, then recovers from it through process termination or resource preemption.</li>\n        <li><b>Deadlock Recovery:</b> This involves strategies to break the deadlock once detected, such as aborting one or more processes to reclaim resources.</li>\n    </ul>\n\n    <h3>Deadlock Prevention Techniques</h3>\n\n    <h4>One-Shot Allocation (All-or-Nothing Allocation)</h4>\n    <p>One-Shot Allocation requires a process to request and be allocated all the resources it will need at once. If any resources are not available, none are allocated, and the process waits until all can be provided together. This prevents the hold and wait condition.</p>\n    <p>Advantages:</p>\n    <ul>\n        <li>Simplifies resource allocation logic.</li>\n        <li>Ensures that a process will not enter a wait state after starting.</li>\n    </ul>\n    <p>Disadvantages:</p>\n    <ul>\n        <li>Leads to low resource utilization and possible starvation.</li>\n        <li>Difficult to predict and request all needed resources in advance.</li>\n    </ul>\n\n    <h4>Preemptive Resource Allocation</h4>\n    <p>Preemptive Resource Allocation allows the system to forcibly take back resources from processes to avoid deadlock. If a process requests more resources while holding some, the system may temporarily preempt these resources to allocate them to other waiting processes.</p>\n    <p>Advantages:</p>\n    <ul>\n        <li>Reduces the likelihood of deadlock by breaking hold-and-wait conditions.</li>\n        <li>Improves resource utilization by reallocating resources dynamically.</li>\n    </ul>\n    <p>Disadvantages:</p>\n    <ul>\n        <li>Complex to implement and manage, requiring mechanisms to save and restore process states.</li>\n        <li>May lead to overhead and performance issues due to frequent preemptions.</li>\n    </ul>\n\n    <h3>Dining Philosophers Problem</h3>\n    <p>The Dining Philosophers Problem is a classic synchronization problem that illustrates the challenges of allocating limited resources among competing processes without causing deadlock. The problem is set up as follows:</p>\n    <ul>\n        <li>Five philosophers sit around a circular table, alternating between thinking and eating.</li>\n        <li>Each philosopher has a plate of pasta, and there is one fork between each pair of adjacent philosophers.</li>\n        <li>To eat, a philosopher needs both forks, and can only pick up one at a time.</li>\n    </ul>\n    <p>The deadlock scenario arises when each philosopher simultaneously picks up the fork to their left.</p>\n    <img src=\"https://static.takeuforward.org/content/-wpKR8nyr\" alt=\"Dining Philosophers Problem\", width=400>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>\n"
      },
      {
        "id": "Banker's-algorithm",
        "sl_no_in_step": 9,
        "title": "Banker's algorithm",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/bankers-algorithm",
        "content": "<p>The <b>Banker's Algorithm</b>, developed by Edsger W. Dijkstra, is a deadlock avoidance algorithm that ensures a safe sequence of resource allocation, preventing system deadlocks. Similar to a banking system where a banker allocates funds to clients, it operates under the assumption that there are multiple resources available and each process has a maximum claim for each resource.</p>\n    <p>The algorithm evaluates whether it is safe to grant a resource request by simulating the allocation and checking if the system can still fulfill the maximum resource needs of all processes without leading to a deadlock. If a safe sequence of resource allocation exists, the request is granted; otherwise, the process must wait.</p>\n    <img src=\"https://static.takeuforward.org/content/-s5iFbk01\" alt=\"Banker's Algorithm\" width=\"400\">\n    \n    <h3>The Algorithm</h3>\n    <p>The core principle of the Banker's Algorithm is to avoid unsafe states where a deadlock would occur. It works by maintaining data on available resources, maximum demands, currently allocated resources, and the remaining needs of each process.</p>\n    <p>When a process requests resources, the algorithm checks if fulfilling the request keeps the system in a safe state. This is done by temporarily allocating the resources and then determining if the remaining resources are sufficient for the maximum needs of the other processes. If the system remains in a safe state, the allocation is confirmed; otherwise, the request is denied and the process must wait.</p>\n   \n        <img src=\"https://static.takeuforward.org/content/-yBo505lE\" alt=\"Safety Check\" width=\"400\">\n    <h3>How Resources Are Allocated</h3>\n    <ul>\n        <li><b>Initialisation</b>: Set up matrices and vectors for available resources, maximum demands, allocated resources, and remaining needs.</li>\n        <li><b>Request Handling</b>: When a process requests resources, check if the request is less than its remaining need and the available resources.</li>\n        <li><b>Safety Check</b>: Temporarily allocate the requested resources to the process and update the available resources.</li>\n        <li><b>Determine Safe State</b>: Simulate the allocation to check if the remaining resources can satisfy the needs of all other processes. This involves finding at least one sequence of process completion that leads to all processes eventually receiving their maximum required resources.</li>\n        <li><b>Grant or Deny</b>: If a safe sequence exists, confirm the allocation. If not, revert to the previous state and make the process wait.</li>\n    </ul>\n    <img src=\"https://static.takeuforward.org/content/-EIYalS4O\" alt=\"Resource Allocation Process\" width=\"400\">\n\n    <h3>Example</h3>\n    <p>Consider a system with three types of resources (A, B, C) and three processes (P1, P2, P3). Suppose the available resources are [3, 3, 2], and the maximum demands and current allocations are:</p>\n    <ul>\n        <li><b>P1</b>: Max [7, 5, 3], Allocated [0, 1, 0]</li>\n        <li><b>P2</b>: Max [3, 2, 2], Allocated [2, 0, 0]</li>\n        <li><b>P3</b>: Max [9, 0, 2], Allocated [3, 0, 2]</li>\n    </ul>\n    <p>When P1 requests [1, 0, 2], the algorithm checks if this request can be granted:</p>\n    <ol>\n        <li><b>Check request validity</b>: [1, 0, 2] ≤ [7, 4, 3] (remaining need for P1), and [1, 0, 2] ≤ [3, 3, 2] (available).</li>\n        <li><b>Safety check</b>: Temporarily allocate the resources to P1 and update available to [2, 3, 0].</li>\n        <li><b>Simulate completion</b>: Check if the system can fulfill the maximum needs of P2 and P3 with the remaining resources. If a safe sequence is found, the request is granted.</li>\n    </ol>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      }
    ]
  },
  {
    "step_no": 3,
    "topic": "Memory Management",
    "data": [
      {
        "id": "Memory-management-and-Techniques",
        "sl_no_in_step": 1,
        "title": "Memory management and Techniques(First Fit, Best Fit, Worst Fit)",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/memory-management-techniques",
        "content": "    <p><b>Memory Management</b> involves handling and optimizing the use of the main memory. Its primary objective is to ensure that programs run efficiently by allocating memory spaces dynamically, keeping track of each byte in a computer's memory, and maximizing system performance.</p>\n    <p>Effective memory management is essential to avoid scenarios like memory leaks, which can degrade system performance over time, or memory corruption, which can lead to system crashes and unpredictable behavior.</p>\n\n    <h3>Memory Allocation Strategies</h3>\n    <p>Memory allocation strategies are methods used to assign memory blocks to various programs and processes within a computer system. The most common strategies include static allocation, where memory is assigned at compile time, and dynamic allocation, where memory is allocated at runtime.</p>\n    <p>These strategies are crucial in managing the limited resource of memory, ensuring that all running applications receive the necessary resources while reducing wastage. Dynamic memory allocation techniques, such as heap allocation and stack allocation, allow for flexible and efficient use of memory, adapting to the changing needs of applications.</p>\n\n    <h3>Fragmentation: Internal and External</h3>\n    <img src=\"https://static.takeuforward.org/content/-rJ7DL3z1\" alt=\"Fragmentation\">\n    <p>Fragmentation in memory management refers to the inefficient use of memory that reduces the amount of usable memory. It occurs in two forms: internal and external fragmentation.</p>\n    <ul>\n        <li><b>Internal Fragmentation</b>: Happens when allocated memory blocks have unused space within them, typically because the memory requested is slightly smaller than the allocated block size.</li>\n        <li><b>External Fragmentation</b>: Occurs when free memory is divided into small, non-contiguous blocks, making it challenging to find a block large enough to satisfy a memory request, despite having sufficient total free memory.</li>\n    </ul>\n    <p>Both types of fragmentation can significantly impact system performance by reducing the effective memory available for applications.</p>\n\n    <h3>First Fit Algorithm</h3>\n    <p>The <b>First Fit</b> algorithm is a memory allocation strategy that assigns the first available memory block that is large enough to accommodate the requested size. It scans memory from the beginning and chooses the first sufficiently large block it encounters. This approach is simple and generally fast since it minimizes the search time.</p>\n    <p>However, it can lead to fragmentation over time, as the algorithm tends to leave small unusable gaps in the memory, especially when there are numerous allocation and deallocation operations.</p>\n    <img src=\"https://static.takeuforward.org/content/-rE2ejsAS\" alt=\"First Fit\" >\n\n    <h3>Best Fit Algorithm</h3>\n    <p>The <b>Best Fit</b> algorithm allocates memory by searching for the smallest free block that is large enough to satisfy the memory request. This strategy aims to reduce wasted space by using the most appropriately sized block available.</p>\n    <img src=\"https://static.takeuforward.org/content/-QlfLB8cO\" alt=\"Best Fit\" >\n    <p>While this method can reduce internal fragmentation, it often increases external fragmentation because it tends to leave very small gaps that are difficult to use for future requests. Additionally, the Best Fit algorithm may involve a more extensive search, making it potentially slower than other allocation methods.</p>\n\n    <h3>Worst Fit Algorithm</h3>\n    <p>The <b>Worst Fit</b> algorithm allocates memory by searching for the largest available block that can satisfy the memory request. The idea is to leave the largest possible leftover block after allocation, which might be more useful for subsequent requests.</p>\n    <img src=\"https://static.takeuforward.org/content/-KlSqNQEx\" alt=\"Worst Fit\">\n    <p>This approach can help reduce external fragmentation by avoiding the creation of small, unusable gaps. However, it can lead to inefficient memory usage if the largest blocks are not split effectively, and it often requires a longer search time compared to simpler algorithms like First Fit.</p>\n\n    <h3>Buddy System Allocation</h3>\n    <p>The <b>Buddy System</b> is a memory allocation technique that divides memory into partitions of size that are powers of two. When a request is made, the system splits the available block into smaller \"buddies\" until it finds a block size that best fits the request.</p>\n    <img src=\"https://static.takeuforward.org/content/-K_sI_JP-\" alt=\"Buddy System\" >\n    <p>If the allocated block is later freed, and its buddy is also free, the system merges them back into a larger block. This approach helps manage fragmentation by simplifying the coalescing of free memory blocks and maintaining a structured method for splitting and merging memory. The Buddy System balances the need for efficiency and simplicity in managing memory allocation.</p>\n\n    <h3>Slab Allocation</h3>\n    <p><b>Slab Allocation</b> is a memory management technique used primarily in kernel memory allocation, where efficiency and speed are crucial. It involves pre-allocating chunks of memory, called slabs, for objects of the same size. When an object is needed, it is taken from a pre-allocated slab, and when it is no longer needed, it is returned to the slab.</p>\n    <p>This method reduces fragmentation and overhead by minimizing frequent allocations and deallocations, providing a fast and predictable allocation pattern. Slab allocation is particularly effective in environments where objects of fixed sizes are frequently created and destroyed, such as in operating systems.</p>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Virtual-Memory",
        "sl_no_in_step": 2,
        "title": "Virtual Memory",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/virtual-memory",
        "content": "<p><b>Virtual memory</b> is a memory management technique that provides an \"idealized abstraction of the storage resources\" that are actually available on a given machine, creating the illusion to users of a very large (main) memory.</p>\n    <p>This is achieved by using both the computer's physical memory (RAM) and a portion of the storage device (hard disk or SSD). When a program requires more memory than what is physically available, the operating system temporarily transfers inactive parts of the program's data to a pre-configured space on the disk, known as the swap space or page file.</p>\n    <p>This allows for more efficient and flexible use of memory, enabling systems to run larger applications and multitask more effectively.</p>\n    <img src=\"https://static.takeuforward.org/content/-D2nyixJ2\" alt=\"Virtual Memory\" >\n\n    <h3>Advantages of Virtual Memory</h3>\n    <ul>\n        <li>Increases the effective memory capacity.</li>\n        <li>Allows larger applications to run on systems with limited physical RAM.</li>\n        <li>Enables more programs to run simultaneously without exhausting physical memory.</li>\n        <li>Creates a layer of abstraction between physical hardware and applications.</li>\n        <li>Improves system stability by preventing applications from interfering with each other’s memory.</li>\n        <li>Enhances security and reliability, ensuring one process’s crash or misbehavior doesn’t affect others.</li>\n    </ul>\n\n    <h3>Paging vs Segmentation</h3>\n    <p>Paging and segmentation are two approaches to memory management in operating systems.</p>\n    <img src=\"https://static.takeuforward.org/content/-mS_bqqS3\" alt=\"Paging vs Segmentation\" >\n    <p><b>Paging</b> divides the virtual memory into fixed-size blocks called pages, while the physical memory is divided into blocks of the same size called frames. This simplifies memory allocation and management, as any page can be loaded into any frame, eliminating fragmentation issues.</p>\n    <p><b>Segmentation</b>, on the other hand, divides the memory into variable-sized segments based on logical divisions, such as functions, arrays, or modules. Each segment can grow or shrink independently, providing a more logical way of memory allocation that matches the program structure but may suffer from fragmentation and complex memory management.</p>\n\n    <h3>Comparison: Paging vs Segmentation</h3>\n    <table border=\"1\" cellspacing=\"0\" cellpadding=\"8\">\n        <thead>\n            <tr>\n                <th>Feature</th>\n                <th>Paging</th>\n                <th>Segmentation</th>\n            </tr>\n        </thead>\n        <tbody>\n            <tr>\n                <td><b>Memory Division</b></td>\n                <td>Fixed-sized blocks called pages</td>\n                <td>Variable-size blocks called segments</td>\n            </tr>\n            <tr>\n                <td><b>Physical Memory</b></td>\n                <td>Divided into frames of the same size as pages</td>\n                <td>Divided into segments of varying sizes</td>\n            </tr>\n            <tr>\n                <td><b>Address Structure</b></td>\n                <td>Single-level address with page number and offset</td>\n                <td>Two-level address with segment number and offset</td>\n            </tr>\n            <tr>\n                <td><b>Fragmentation</b></td>\n                <td>Eliminates external fragmentation, may cause internal fragmentation within pages</td>\n                <td>Can lead to external fragmentation, but no internal fragmentation</td>\n            </tr>\n            <tr>\n                <td><b>Logical Division</b></td>\n                <td>Divides memory without considering logical structure</td>\n                <td>Divides memory according to logical structure (e.g., functions, arrays)</td>\n            </tr>\n            <tr>\n                <td><b>Ease of Management</b></td>\n                <td>Simpler due to fixed-size pages</td>\n                <td>More complex due to variable-size segments</td>\n            </tr>\n            <tr>\n                <td><b>Protection and Sharing</b></td>\n                <td>Easier to manage protection and sharing at page level</td>\n                <td>More intuitive for logical groupings but complex</td>\n            </tr>\n            <tr>\n                <td><b>Memory Access</b></td>\n                <td>Uniform size makes access time consistent</td>\n                <td>Variable sizes can lead to varying access times</td>\n            </tr>\n            <tr>\n                <td><b>Use Case</b></td>\n                <td>Commonly used in modern operating systems</td>\n                <td>Less common, used for specific applications requiring logical division</td>\n            </tr>\n        </tbody>\n    </table>\n\n    <h3>Logical and Physical Address Space</h3>\n    <img src=\"https://static.takeuforward.org/content/-o-apPvom\" alt=\"Logical and Physical Address Space\" >\n    <p><b>Logical Address Space</b> is also known as the virtual address space, is a set of addresses that a program can use to access memory. These addresses are generated by the CPU during program execution and are independent of the actual physical memory addresses. The logical address space provides an abstraction layer, allowing programs to use a continuous and consistent range of addresses, which simplifies programming and enhances portability.</p>\n    <p><b>Physical Address Space</b> is the actual location in the computer’s physical memory (RAM). These addresses are used by the memory management unit (MMU) to access data stored in the system's hardware memory. The physical address space is finite and directly corresponds to the physical hardware installed in the machine.</p>\n    <p>Logical Address space is a virtualized abstraction used by the software, while the physical address space is the actual memory hardware. These combined allow the operating system to manage memory more efficiently, providing each process with its own logical address space, mapped onto the physical memory by the Memory Management Unit.</p>\n\n    <h3>Address Translation</h3>\n    <p><b>Address Translation</b> is the process of converting logical addresses into physical addresses. This translation is typically handled by the memory management unit (MMU) and involves the following steps:</p>\n    <ol>\n        <li>The CPU generates a logical address during program execution. The address is divided into two parts: the page number and offset.</li>\n        <li>The MMU uses the page number to index into the page table, a data structure that maintains the mapping between logical pages and physical frames. Each entry in the page table corresponds to a page frame number in physical memory.</li>\n        <li>The frame number obtained from the page table is combined with the offset to form the complete physical address. This address points to the exact location in physical memory where the data is stored.</li>\n        <li>The physical address is used to access the data in the physical memory, completing the translation process.</li>\n    </ol>\n    <p>Address translation allows the operating system to provide each process with its own logical address space, ensuring that processes do not interfere with each other’s memory. It also enables features like paging and segmentation, which help in efficient memory management and protection.</p>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Contiguous-allocation,-Paging-and-Segmentation",
        "sl_no_in_step": 3,
        "title": "Contiguous allocation , Paging and Segmentation",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/contiguous-allocation-paging-segmentation",
        "content": "<h2>Contiguous Memory Allocation</h2>\n    <p>\n        Contiguous Memory Allocation is a memory management technique where each process is allocated a single continuous block of memory. This approach is simple and efficient in terms of access speed because the CPU can easily calculate the address of any location within the block.\n    </p>\n    <div class=\"image-container\">\n        <img src=\"https://static.takeuforward.org/content/-hMawFssQ\" alt=\"Contiguous Memory Allocation\">\n    </div>\n    <p>\n        However, contiguous memory allocation suffers from fragmentation. External fragmentation occurs when there are sufficient total free memory spaces, but these are not contiguous, making it difficult to allocate large blocks of memory. Internal fragmentation happens when allocated memory blocks are slightly larger than the requested memory, leading to wasted space.\n    </p>\n    <p>\n        Additionally, the process of fitting processes into memory can be complex, requiring strategies like first-fit, best-fit, or worst-fit to manage available memory effectively.\n    </p>\n\n    <h2>Paging</h2>\n    <p>\n        Paging is a memory management scheme that eliminates the need for contiguous allocation of physical memory. In paging, the physical memory is divided into fixed-size blocks called frames, and the logical memory (used by processes) is divided into blocks of the same size called pages. When a process is loaded into memory, its pages can be loaded into any available memory frames, and a page table keeps track of the mapping between the process’s pages and the physical frames.\n    </p>\n    <div class=\"image-container\">\n        <img src=\"https://static.takeuforward.org/content/-2KpkaVY1\" alt=\"Paging\">\n    </div>\n    <p>\n        This allows the physical address space to be non-contiguous, which solves the problem of external fragmentation and makes it easier to allocate memory efficiently. Paging also simplifies memory management by eliminating the need for complex allocation algorithms.\n    </p>\n\n    <h3>Page Table Structure</h3>\n    <p>\n        The page table is a data structure used in paging to store the mapping between virtual addresses and physical addresses. Each entry in a page table corresponds to a page of the process's logical address space and contains the address of the frame in physical memory where the page is stored.\n    </p>\n    <div class=\"image-container\">\n        <img src=\"https://static.takeuforward.org/content/-vgiSDSEx\" alt=\"Page Table Structure\">\n    </div>\n    <p>\n        Page tables can be implemented in several ways, including single-level, multi-level, and inverted page tables. A single-level page table is a simple array, but it can become very large if the address space is large.\n    </p>\n\n    <h3>Paging: Advantages and Disadvantages</h3>\n    <ul>\n        <li><strong>Advantages:</strong></li>\n        <ul>\n            <li>Avoids gaps between allocated memory blocks, ensuring efficient use of available memory.</li>\n            <li>Pages can be placed anywhere in physical memory, simplifying memory allocation.</li>\n            <li>Fixed-size pages mean that memory allocation does not require complex algorithms.</li>\n            <li>Allows larger programs to run even with limited physical memory.</li>\n            <li>Virtual memory allows parts of a program to be stored on disk and brought into memory as needed.</li>\n        </ul>\n        <li><strong>Disadvantages:</strong></li>\n        <ul>\n            <li>Each process requires a page table, which can consume a lot of memory for large address spaces.</li>\n            <li>Each memory access requires a lookup in the page table, which can add delay.</li>\n            <li>Unused space in the last page of a process can lead to internal fragmentation.</li>\n        </ul>\n    </ul>\n\n    <h2>Segmentation: Concept and Organisation</h2>\n    <p>\n        Segmentation is a memory management technique that divides the process's memory into variable-sized segments, each of which can be a logical unit such as a function, array, or data structure. Unlike paging, segments vary in length, reflecting the logical structure of the process.\n    </p>\n    <div class=\"image-container\">\n        <img src=\"https://static.takeuforward.org/content/-mq-95-02\" alt=\"Segmentation\">\n    </div>\n    <p>\n        Each segment has a segment number and a length, and memory addresses within a segment are specified by an offset from the segment's base address.\n    </p>\n    <p>\n        A segment table keeps track of each segment's base address and length. Segmentation facilitates sharing and protection, as segments can be independently protected and shared between processes, enhancing modularity and security.\n    </p>\n\n    <h3>Segmentation: Advantages and Disadvantages</h3>\n    <ul>\n        <li><strong>Advantages:</strong></li>\n        <ul>\n            <li>Improves program readability and manageability.</li>\n            <li>Each segment can have its own access rights, enhancing security.</li>\n            <li>Reduces wasted space within segments.</li>\n        </ul>\n        <li><strong>Disadvantages:</strong></li>\n        <ul>\n            <li>Can suffer from external fragmentation.</li>\n            <li>Requires more complex algorithms for allocation and deallocation.</li>\n        </ul>\n    </ul>\n\n    <h2>Paging vs Segmentation</h2>\n    <table>\n        <tr>\n            <th>Feature</th>\n            <th>Paging</th>\n            <th>Segmentation</th>\n        </tr>\n        <tr>\n            <td>Memory Division</td>\n            <td>Fixed-sized blocks called pages</td>\n            <td>Variable-size blocks called segments</td>\n        </tr>\n        <tr>\n            <td>Physical Memory</td>\n            <td>Divided into frames of the same size as pages</td>\n            <td>Divided into segments of varying sizes</td>\n        </tr>\n        <tr>\n            <td>Address Structure</td>\n            <td>Single-level address with page number and offset</td>\n            <td>Two-level address with segment number and offset</td>\n        </tr>\n        <tr>\n            <td>Fragmentation</td>\n            <td>Eliminates external fragmentation, may cause internal fragmentation within pages</td>\n            <td>Can lead to external fragmentation, but no internal fragmentation</td>\n        </tr>\n        <tr>\n            <td>Logical Division</td>\n            <td>Divides memory without considering logical structure</td>\n            <td>Divides memory according to logical structure (e.g., functions, arrays)</td>\n        </tr>\n        <tr>\n            <td>Ease of Management</td>\n            <td>Simpler due to fixed-size pages</td>\n            <td>More complex due to variable-size segments</td>\n        </tr>\n        <tr>\n            <td>Protection and Sharing</td>\n            <td>Easier to manage protection and sharing at page level</td>\n            <td>More intuitive for logical groupings but complex</td>\n        </tr>\n        <tr>\n            <td>Memory Access</td>\n            <td>Uniform size makes access time consistent</td>\n            <td>Variable sizes can lead to varying access times</td>\n        </tr>\n        <tr>\n            <td>Use Case</td>\n            <td>Commonly used in modern operating systems</td>\n            <td>Less common, used for specific applications requiring logical division</td>\n        </tr>\n    </table>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Dynamic-binding",
        "sl_no_in_step": 4,
        "title": "Dynamic binding",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/dynamic-binding",
        "content": "   <p>\n        Dynamic Binding, also known as <b>late binding</b>, is a programming mechanism where the method to be called is determined at runtime, not during compile-time. It is a key part of <b>polymorphism</b> in object-oriented programming, allowing objects to be handled more generically.\n    </p>\n    <p>\n        For example, a base class pointer can refer to objects of derived classes, and the correct method is chosen based on the object's type at runtime.\n    </p>\n    <p>\n        <b>Benefits of Dynamic Binding:</b>\n    </p>\n    <ul>\n        <li>Enhances flexibility and extensibility, as new classes and methods can be added with minimal changes.</li>\n        <li>Supports modular and maintainable code, which is crucial for modern software development.</li>\n    </ul>\n    <img src=\"https://static.takeuforward.org/content/-W5nfHxAw\" alt=\"Dynamic Binding Image\">\n\n    <h3>Dynamic Linking vs Dynamic Loading</h3>\n    <p>\n        <b>Dynamic Linking</b> links required libraries or modules to a program at runtime, instead of during compile-time. The compiler includes references to shared libraries, and the operating system loads these libraries when the program starts.\n    </p>\n    <p>\n        <b>Dynamic Loading</b> is when an application explicitly loads or unloads libraries at runtime. This is often used for loading plugins or optional modules.\n    </p>\n    <img src=\"https://static.takeuforward.org/content/-X_n4Ztlr\" alt=\"Dynamic Linking and Loading\">\n\n    <h3>Differences between Dynamic Linking and Dynamic Loading</h3>\n    <ul>\n        <li><b>Dynamic Linking:</b> Automatically managed by the OS at program start, shares library code across multiple programs.</li>\n        <li><b>Dynamic Loading:</b> Controlled by the application, ideal for plugins and modules that load based on user needs.</li>\n        <li>Dynamic Linking saves disk space by not duplicating library code.</li>\n        <li>Dynamic Loading is highly modular, allowing for precise control over loaded modules.</li>\n        <li>Dynamic Linking may have version issues (\"DLL Hell\"), while Dynamic Loading reduces these issues.</li>\n    </ul>\n\n    <h3>Late Binding vs Early Binding</h3>\n    <p>\n        <b>Late binding</b>, or dynamic binding, happens when the method or function to be called is determined at runtime. This is common in object-oriented programming using virtual functions or interfaces.\n    </p>\n    <p>\n        <b>Early binding</b>, or static binding, happens when the method or function is determined during compilation. The method to be called is known at compile-time, which results in faster execution.\n    </p>\n    <img src=\"https://static.takeuforward.org/content/-xu6ejV0G\" alt=\"Late Binding vs Early Binding Image\">\n\n    <h3>Comparison between Late Binding and Early Binding</h3>\n    <ul>\n        <li><b>Late Binding:</b> Determined at runtime, supports polymorphism and flexibility, but may have runtime errors.</li>\n        <li><b>Early Binding:</b> Determined at compile-time, faster due to no method lookup overhead, but less flexible.</li>\n        <li>Late binding is used in languages like Python and JavaScript.</li>\n        <li>Early binding is common in languages like Java and C# for performance-critical applications.</li>\n    </ul>\n\n    <h3>Benefits of Dynamic Binding</h3>\n    <ul>\n        <li><b>Greater Flexibility:</b> It allows methods to be chosen based on the actual runtime context, adapting to different scenarios.</li>\n        <li><b>Supports Polymorphism:</b> Enables different classes to be treated the same way, which allows code reuse and easier extensibility.</li>\n        <li><b>Simplifies Code Maintenance:</b> New methods can be added without changing existing code.</li>\n        <li><b>Useful in Dynamic Languages:</b> Languages that determine types at runtime use dynamic binding for flexibility.</li>\n    </ul>\n\n    <h3>Drawbacks of Dynamic Binding</h3>\n    <ul>\n        <li><b>Runtime Overhead:</b> Method lookup can slow down execution, which may be a concern in performance-critical applications.</li>\n        <li><b>Errors Surface During Runtime:</b> Debugging can be harder since errors are found when the program runs.</li>\n        <li><b>Complex Code Management:</b> Managing dynamic method resolution adds complexity, making code harder to understand in large projects.</li>\n        <li><b>Risk of Type Errors:</b> In dynamically typed languages, it’s easier to call methods incorrectly, leading to runtime errors.</li>\n    </ul>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Page-Fault",
        "sl_no_in_step": 5,
        "title": "Page Fault",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/page-fault",
        "content": "<p>A page fault is an exception that occurs when a program accesses a memory page that is not currently present in physical memory (RAM). In modern operating systems that use virtual memory management, memory is divided into fixed-size pages, and only the pages that are actively being used by a process are kept in physical memory. When a program attempts to access a page that is not in RAM, a page fault occurs, triggering the operating system to load the required page from secondary storage (such as a hard disk or SSD) into physical memory.</p>\n    <img src=\"https://static.takeuforward.org/content/-vgiSDSEx\" alt=\"Page Fault Illustration\">\n\n    <p>Page faults are a fundamental part of virtual memory systems, allowing programs to address more memory than is physically available. They enable efficient use of memory resources by swapping pages in and out of physical memory as needed, based on the demands of running processes. While page faults incur some overhead due to the need to access slower secondary storage, they are essential for providing the illusion of a larger memory space to processes than actually exists in physical memory.</p>\n\n    <h3>Causes of Page Faults</h3>\n    <ul>\n        <li>\n            <b>Demand Paging:</b> Most modern operating systems use demand paging, where only the pages of memory that are actively being used by a process are loaded into physical memory. As a result, when a process accesses a page that is not currently in RAM, a page fault occurs, and the required page is fetched from disk into memory.\n            <img src=\"https://static.takeuforward.org/content/-gbY_x3Xo\" alt=\"Demand Paging Illustration\">\n        </li>\n        <li>\n            <b>Page Replacement:</b> If physical memory is full and a process needs to bring in a new page, the operating system must choose a page to evict from memory to make space. This involves selecting a victim page for replacement, often based on algorithms like Least Recently Used (LRU) or First-In, First-Out (FIFO).\n        </li>\n        <li>\n            <b>Copy-on-Write:</b> Some systems use copy-on-write mechanisms, where multiple processes can share the same memory page until one of them attempts to modify it. When a modification occurs, the page is copied, leading to a page fault for the modifying process.\n        </li>\n    </ul>\n\n    <h3>Handling Page Faults</h3>\n    <img src=\"https://static.takeuforward.org/content/-DMffqD9w\" alt=\"Handling Page Faults Illustration\">\n    <p>When a page fault occurs, the operating system intervenes to handle the exception and ensure that the required page is brought into physical memory. The steps involved in handling a page fault are:</p>\n    <ol>\n        <li><b>Page Table Lookup:</b> The operating system consults the page table of the process to determine whether the accessed memory address corresponds to a valid page in physical memory or if it has been swapped out to disk.</li>\n        <li><b>Disk Access:</b> If the required page is not in physical memory, the operating system initiates a disk access to read the required page from secondary storage into a free page frame in physical memory.</li>\n        <li><b>Updating Page Table:</b> Once the required page is loaded into physical memory, the operating system updates the page table entry for the accessed memory address to indicate that the page is now resident in RAM.</li>\n        <li><b>Resuming Execution:</b> Finally, the operating system restarts the instruction that caused the page fault, allowing the process to continue its execution with the required page now available in physical memory.</li>\n    </ol>\n\n    <h3>Impact of Page Faults on System Performance</h3>\n    <p>Page faults can have a significant impact on system performance, primarily due to the latency introduced by disk accesses compared to accessing data from physical memory. When a page fault occurs, the CPU must wait for the required page to be fetched from secondary storage, leading to a temporary pause in program execution known as a \"page fault stall.\"</p>\n\n    <p>Frequent page faults can result in decreased overall system performance, as the CPU spends more time waiting for data to be fetched from disk rather than executing program instructions. To mitigate the impact of page faults on performance, operating systems employ various techniques, such as:</p>\n    <ul>\n        <li><b>Optimising page replacement algorithms</b></li>\n        <li><b>Utilising disk caching</b></li>\n        <li><b>Employing prefetching strategies</b> to anticipate and load pages into memory before they are needed</li>\n    </ul>\n\n    <p>Additionally, system administrators may tune system parameters, such as the size of the page file or swap space, to balance memory usage and disk access latency. Overall, managing page faults effectively is crucial for maintaining system performance and ensuring efficient utilisation of memory resources.</p>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Page-replacement-algorithms",
        "sl_no_in_step": 6,
        "title": "Page replacement algorithms( LRU, Optimal, FIFO)",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/page-replacement-algorithm",
        "content": "    <p>Page replacement is a key component of virtual memory management in modern operating systems. It involves the process of selecting a page in physical memory to be evicted or swapped out to secondary storage (such as a hard disk or SSD) when a new page needs to be brought into memory. Page replacement is necessary when physical memory becomes full, and a new page needs to be loaded into memory, but there are no free page frames available.</p>\n\n    <p>When a page fault occurs, indicating that a requested page is not currently in physical memory, the operating system initiates a page replacement process to make space for the required page. The page replacement algorithm is responsible for selecting the victim page to be evicted from memory, typically based on a predefined policy.</p>\n    <img src=\"https://static.takeuforward.org/content/-DMffqD9w\" alt=\"Page Replacement Process Illustration\">\n\n    <p>Page replacement directly impacts system performance, as it involves accessing slower secondary storage to swap pages in and out of physical memory. Frequent page replacement can lead to increased disk I/O and latency, which can degrade overall system performance. Therefore, selecting an efficient page replacement algorithm is crucial for optimising system performance and minimising the impact of page faults on system responsiveness. Additionally, system designers may employ techniques such as prefetching, caching, or optimising disk access to mitigate the performance impact of page replacement and improve overall system efficiency.</p>\n\n    <h3>Least Recently Used (LRU) Algorithm</h3>\n    <p>The Least Recently Used (LRU) algorithm is a page replacement policy used in virtual memory management. It operates on the principle that pages that have been least recently used are the ones least likely to be used in the near future. When a page needs to be replaced due to a page fault, the LRU algorithm selects the page in memory that has not been accessed for the longest period and replaces it with the new page.</p>\n\n    <p>LRU can be implemented using various data structures, such as linked lists or hash maps, to track the order of page accesses. Each time a page is accessed, it is moved to the front of the list or marked as most recently used. When a page fault occurs, the algorithm selects the page at the end of the list or the least recently used page for replacement.</p>\n    <img src=\"https://static.takeuforward.org/content/-c-RrrZRM\" alt=\"LRU Algorithm Illustration\">\n\n    <p>LRU is relatively simple to implement and often performs well in practice, especially when there is good temporal locality in memory access patterns.</p>\n\n    <p>The main drawback of the LRU algorithm is its high implementation complexity and the overhead of maintaining the access history for every page in memory. Additionally, LRU may suffer from \"thrashing\" in scenarios where the working set of a process exceeds the available physical memory, leading to frequent page faults and poor performance.</p>\n\n    <h3>Optimal Algorithm</h3>\n    <p>The Optimal algorithm, also known as the MIN or MINOPT algorithm, is a theoretical page replacement policy that selects the page for replacement that will not be accessed for the longest time in the future. In other words, it replaces the page whose next access is farthest in the future.</p>\n    <img src=\"https://static.takeuforward.org/content/-HP4UMYLV\" alt=\"Optimal Algorithm Illustration\">\n\n    <p>The Optimal algorithm requires knowledge of future page access patterns, which is usually not feasible in practice. Therefore, it is often used as a benchmark for comparing the performance of other page replacement algorithms rather than as a practical solution.</p>\n\n    <p>Optimal algorithm provides the lowest possible page fault rate among all page replacement algorithms, as it always selects the optimal page to replace. The main drawback of the Optimal algorithm is its impracticality for real-world systems, as it requires knowledge of future page accesses, which is generally not available. Therefore, it is primarily used as a reference point for evaluating the performance of other algorithms.</p>\n\n    <h3>First-In-First-Out (FIFO) Algorithm</h3>\n    <p>The First-In-First-Out (FIFO) algorithm is a simple page replacement policy that replaces the oldest page in memory when a page fault occurs. It operates on the principle of a queue, where the page that was brought into memory earliest is the first one to be replaced.</p>\n\n    <p>FIFO is typically implemented using a queue data structure, where new pages are added to the back of the queue, and the page at the front of the queue (the oldest page) is replaced when a page fault occurs.</p>\n\n    <p>FIFO is easy to implement and requires minimal bookkeeping, making it suitable for systems with limited resources or where simplicity is prioritised over performance.</p>\n\n    <p>FIFO suffers from the \"Belady's Anomaly,\" where increasing the number of page frames may lead to an increase in page faults rather than a decrease. This anomaly occurs because the FIFO algorithm does not consider the frequency of page accesses or the temporal locality of memory references.</p>\n\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Belady's-Anomaly",
        "sl_no_in_step": 7,
        "title": "Belady's Anomaly",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/belady-anomaly",
        "content": " <p>Belady's Anomaly is a phenomenon observed in page replacement algorithms, where increasing the number of page frames allocated to a process can unexpectedly lead to an increase in the number of page faults, rather than a decrease as expected. It challenges the intuitive notion that providing more memory resources should result in better performance by reducing the frequency of page faults.</p>\n    <img src=\"https://static.takeuforward.org/content/-vIa-z8GE\" alt=\"Belady's Anomaly Illustration\">\n\n    <p>Belady's Anomaly occurs when a page replacement algorithm, such as First-In-First-Out (FIFO), replaces a page that will be needed soon, despite having additional free page frames available. This anomaly highlights the limitations of certain page replacement policies that do not consider the future access patterns of pages. In other words, increasing the number of page frames can result in pages being evicted prematurely, leading to more frequent page faults.</p>\n\n    <h4>Example</h4>\n    <p>Consider a scenario where a process accesses a sequence of memory pages. With a smaller number of page frames available, the page replacement algorithm may evict pages that are not immediately needed to make space for incoming pages, resulting in a certain number of page faults. However, increasing the number of page frames may unexpectedly lead to more page faults, as the algorithm now has more options to choose from and may evict pages that would have been needed in the near future.</p>\n\n    <h4>Causes</h4>\n    <ul>\n        <li><strong>Page Replacement Policies:</strong> The choice of page replacement algorithm significantly influences the occurrence of Belady's Anomaly. Algorithms like FIFO (First-In-First-Out) do not consider the future access patterns of pages and may evict pages that will be needed soon, leading to an increase in page faults when the number of page frames is increased.</li>\n\n        <li><strong>Lack of Future Access Prediction:</strong> Many page replacement algorithms, including FIFO, are based solely on past page access patterns. They do not take into account future page accesses or the temporal locality of memory references, making it difficult to predict which pages will be needed in the near future.</li>\n\n        <li><strong>Allocation of Additional Memory:</strong> Increasing the number of page frames allocated to a process can exacerbate Belady's Anomaly if the page replacement algorithm is not designed to handle the additional memory effectively. The algorithm may evict pages that would have been needed soon, resulting in more frequent page faults despite the availability of additional memory resources.</li>\n    </ul>\n    <img src=\"https://static.takeuforward.org/content/-KZ2Pb28W\" alt=\"Illustration of Belady's Anomaly in Action\">\n\n    <h4>Implications of Belady’s Anomaly</h4>\n    <ul>\n        <li><strong>Unexpected Performance Degradation:</strong> Belady's Anomaly can lead to unexpected performance degradation in virtual memory systems, where increasing the amount of allocated memory does not result in the expected reduction in page faults. This can be particularly problematic in scenarios where system administrators allocate additional memory to improve performance, only to find that it has the opposite effect.</li>\n\n        <li><strong>Challenges in System Optimization:</strong> Belady's Anomaly presents challenges in optimising virtual memory systems for performance. It underscores the importance of selecting page replacement algorithms carefully and designing them to minimise future page faults rather than simply replacing the oldest page in memory. Mitigating Belady's Anomaly often requires using more sophisticated page replacement policies, such as Least Recently Used (LRU) or Optimal algorithms, which consider the temporal locality of memory references and aim to minimise future page faults.</li>\n\n        <li><strong>Importance of System Tuning:</strong> System administrators must be aware of Belady's Anomaly when tuning virtual memory settings and allocating memory resources. They should carefully monitor system performance and adjust memory allocation and page replacement policies to mitigate the impact of Belady's Anomaly and improve overall system efficiency. Additionally, system designers may explore adaptive replacement policies that dynamically adjust the page replacement strategy based on observed access patterns to address the challenges posed by Belady's Anomaly and optimise system performance.</li>\n    </ul>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Thrashing",
        "sl_no_in_step": 8,
        "title": "Thrashing",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/thrashing",
        "content": "<p>Thrashing refers to a state in a computer system where excessive paging activity occurs, leading to a significant decrease in overall system performance. In a thrashing scenario, the system spends more time swapping pages between physical memory (RAM) and secondary storage (such as a hard disk or SSD) than executing actual program instructions.</p>\n    <p>As a result, the CPU is heavily burdened with managing the paging activities, leading to a severe degradation in system responsiveness and throughput.</p>\n    <img src=\"https://static.takeuforward.org/content/-xYdDKSo2\" alt=\"Thrashing Illustration\">\n\n    <h4>Causes of Thrashing</h4>\n    <ul>\n        <li><strong>Insufficient Memory:</strong> Thrashing often occurs when the system does not have enough physical memory to accommodate the working sets of active processes. When the demand for memory exceeds available physical memory, the operating system begins swapping pages in and out of memory frequently to make space for active pages, leading to thrashing.</li>\n\n        <li><strong>Overallocation of Memory:</strong> Allocating more memory to processes than is physically available can also trigger thrashing. In this scenario, the operating system may spend excessive time swapping pages between physical memory and secondary storage, trying to accommodate the memory demands of all active processes.</li>\n\n        <li><strong>High Degree of Multiprogramming:</strong> Thrashing can occur in systems with a high degree of multiprogramming, where multiple processes are simultaneously active and competing for limited physical memory resources. If the combined memory demands of all active processes exceed the available physical memory, thrashing may ensue.</li>\n    </ul>\n\n    <h4>Effects of Thrashing on System Performance</h4>\n    <ul>\n        <li><strong>Severe Degradation in Performance:</strong> Thrashing results in a significant degradation in overall system performance, as the CPU spends more time managing paging activities than executing useful program instructions. This leads to sluggish responsiveness and increased latency for user interactions and system operations.</li>\n\n        <li><strong>Increased Disk I/O:</strong> Thrashing increases the frequency of disk I/O operations, as the operating system continuously swaps pages between physical memory and secondary storage. This excessive disk activity can lead to increased wear and tear on storage devices and can degrade system performance further due to the inherent latency associated with disk accesses.</li>\n\n        <li><strong>Poor Resource Utilisation:</strong> Thrashing leads to poor utilisation of system resources, as a significant portion of CPU and disk bandwidth is consumed by paging activities rather than executing user processes. This can result in underutilization of CPU resources and decreased throughput for compute-intensive tasks.</li>\n    </ul>\n\n    <h4>Techniques to Prevent Thrashing</h4>\n    <ul>\n        <li><strong>Optimising Memory Allocation:</strong> Properly sizing memory allocations for processes can help prevent thrashing. System administrators should monitor memory usage and adjust memory allocations to ensure that the working sets of active processes fit within the available physical memory without excessive paging.</li>\n\n        <li><strong>Using Effective Page Replacement Policies:</strong> Employing efficient page replacement policies, such as Least Recently Used (LRU) or Optimal algorithms, can help minimise thrashing by ensuring that the most frequently accessed pages remain in physical memory. These algorithms aim to maximise memory utilisation and minimise page faults, reducing the likelihood of thrashing.</li>\n\n        <li><strong>Limiting Degree of Multiprogramming:</strong> Limiting the number of active processes or the degree of multiprogramming can help prevent thrashing by reducing the overall memory demands on the system. System administrators can adjust system parameters or use workload management techniques to control the number of concurrent processes and prioritise memory allocation for critical tasks.</li>\n\n        <li><strong>Using Memory Management Techniques:</strong> Techniques such as memory paging or segmentation can help optimise memory usage and prevent thrashing by efficiently managing memory allocation and access patterns. These techniques partition memory into smaller units and allocate memory dynamically based on the needs of active processes, reducing the likelihood of memory contention and thrashing.</li>\n    </ul>\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Disk-Scheduling",
        "sl_no_in_step": 9,
        "title": "Disk Scheduling",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/disk-scheduling",
        "content": " <p>Disk scheduling is a crucial aspect of operating system design that deals with efficiently managing the order in which disk I/O requests are serviced by the disk subsystem. In computer systems, data is stored on disk drives, which consist of spinning platters with magnetic surfaces. Disk scheduling algorithms determine the optimal sequence in which these data requests are processed to minimise access latency and maximise throughput.</p>\n    <img src=\"https://static.takeuforward.org/content/-NVZhGBhn\" alt=\"Disk Scheduling Illustration\">\n\n    <h3>Role of Disk Scheduling in Operating Systems</h3>\n    <p>The role of disk scheduling in operating systems is to optimise the utilisation of disk resources and minimise the time required to access data stored on disk drives. Disk I/O operations are typically one of the slowest operations in computer systems due to the mechanical nature of disk drives. Therefore, efficient disk scheduling is essential for improving overall system performance and responsiveness.</p>\n    <p>Disk scheduling algorithms play a critical role in coordinating the access of multiple processes or threads to shared disk resources. By prioritising and organising disk I/O requests, disk scheduling algorithms ensure fair access to disk resources while maximising throughput and minimising access latency. This is particularly important in multi-user or multi-tasking environments where multiple processes may concurrently access disk resources.</p>\n    <img src=\"https://static.takeuforward.org/content/-6LKRq6im\" alt=\"Disk Scheduling Process Illustration\">\n\n    <h3>Factors Influencing Disk Scheduling</h3>\n    <p>Several factors influence the design and selection of disk scheduling algorithms:</p>\n    <ul>\n        <li><strong>Seek Time:</strong> Seek time is the time required for the disk arm to move to the desired track on the disk platter. Disk scheduling algorithms aim to minimise seek time by ordering disk I/O requests to reduce the distance the disk arm needs to travel.</li>\n\n        <li><strong>Rotational Latency:</strong> Rotational latency refers to the time it takes for the desired disk sector to rotate under the disk head once the disk arm is positioned over the correct track. Disk scheduling algorithms may attempt to reduce rotational latency by optimising the order of disk I/O requests to minimise waiting time for the desired sector to rotate into position.</li>\n\n        <li><strong>Transfer Time:</strong> Transfer time is the time required to read or write data once the disk head is positioned over the desired track and sector. While transfer time is primarily influenced by the disk's rotational speed and data transfer rate, disk scheduling algorithms may optimise the order of disk I/O requests to maximise data transfer rates and minimise idle time.</li>\n\n        <li><strong>Concurrency and Fairness:</strong> Disk scheduling algorithms must consider the concurrency and fairness requirements of multiple processes or threads accessing disk resources concurrently. Fairness ensures that all processes have equitable access to disk resources, while concurrency management aims to maximise disk throughput without sacrificing fairness.</li>\n    </ul>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Disk-Scheduling-Algorithms",
        "sl_no_in_step": 10,
        "title": "Disk Scheduling Algorithms",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/disk-scheduling-algorithms",
        "content": "   <h3>First-Come-First-Served (FCFS) Algorithm</h3>\n    <p>In the FCFS algorithm, disk I/O requests are serviced in the order they arrive. The disk arm moves to the requested track and services the request without considering the distance to the next request. FCFS is simple to implement but can lead to high average seek times, especially if there are large variations in seek times between requests.</p>\n    <img src=\"https://static.takeuforward.org/content/-86PSoQla\" alt=\"FCFS Illustration\">\n    <p><strong>Example:</strong> Consider a disk with requests to access tracks 98, 183, 37, 122, 14, and the current position of the disk arm is at track 53. The FCFS algorithm would service the requests in the order they are listed, regardless of the actual distance between tracks.</p>\n    <p><strong>Advantages:</strong> FCFS is easy to implement and ensures fairness, as requests are serviced in the order they are received.</p>\n    <p><strong>Disadvantages:</strong> FCFS can lead to poor performance due to high seek times, especially if there are large variations in seek times between requests.</p>\n\n    <h3>Shortest Seek Time First (SSTF) Algorithm</h3>\n    <p>SSTF selects the request with the shortest seek time from the current position of the disk arm. This algorithm aims to minimise the total seek time by always servicing the nearest request next. SSTF can result in a significant reduction in average seek time compared to FCFS but may suffer from starvation, where requests at the outer tracks are frequently skipped.</p>\n    <img src=\"https://static.takeuforward.org/content/-_24jjkZw\" alt=\"SSTF Illustration\">\n    <p><strong>Example:</strong> Using the same example as before, if the disk arm is at track 53 and the requests are to access tracks 98, 183, 37, 122, and 14, SSTF would service the request to track 37 first, as it has the shortest seek time from track 53.</p>\n    <p><strong>Advantages:</strong> SSTF can significantly reduce average seek time compared to FCFS, leading to improved disk performance.</p>\n    <p><strong>Disadvantages:</strong> SSTF may suffer from starvation, where requests at the outer tracks are frequently skipped in favour of servicing requests closer to the current position of the disk arm.</p>\n\n    <h3>SCAN Algorithm</h3>\n    <p>The SCAN algorithm (also known as the elevator algorithm) moves the disk arm in one direction across the disk, servicing requests along the way. When the arm reaches the end of the disk, it reverses direction and scans back to the other end. SCAN aims to minimise the average seek time by servicing requests in a back-and-forth manner.</p>\n    <img src=\"https://static.takeuforward.org/content/-wJC3sNk_\" alt=\"SCAN Illustration\">\n    <p><strong>Example:</strong> Using the same example, if the disk arm is at track 53 and the requests are to access tracks 98, 183, 37, 122, and 14, SCAN would first service requests in the direction of movement (e.g., tracks 98, 122), then reverse direction and service requests in the other direction (e.g., tracks 37, 14).</p>\n    <p><strong>Advantages:</strong> SCAN can provide a good balance between fairness and performance by servicing requests in a back-and-forth manner.</p>\n    <p><strong>Disadvantages:</strong> SCAN may not be optimal for systems with high variance in request distribution, as requests at the ends of the disk may experience longer wait times.</p>\n\n    <h3>C-SCAN Algorithm</h3>\n    <p>The C-SCAN algorithm is a variant of SCAN that only scans in one direction, servicing requests until it reaches the end of the disk, at which point it jumps back to the beginning of the disk and continues scanning. C-SCAN aims to reduce the variance in service times compared to SCAN by ensuring that all requests are serviced in the same direction.</p>\n    <img src=\"https://static.takeuforward.org/content/-fLYqc0s4\" alt=\"C-SCAN Illustration\">\n    <p><strong>Example:</strong> Using the same example, if the disk arm is at track 53 and the requests are to access tracks 98, 183, 37, 122, and 14, C-SCAN would first service requests in the direction of movement (e.g., tracks 98, 122), then jump back to the beginning of the disk and continue servicing requests in the same direction (e.g., tracks 14, 37).</p>\n    <p><strong>Advantages:</strong> C-SCAN can reduce the variance in service times compared to SCAN by ensuring that all requests are serviced in the same direction.</p>\n    <p><strong>Disadvantages:</strong> C-SCAN may lead to increased average seek times compared to SCAN, especially if there are requests at both ends of the disk.</p>\n\n    <h3>LOOK Algorithm</h3>\n    <p>The LOOK algorithm is a variant of SCAN that services requests only until it reaches the last request in the current direction, at which point it reverses direction without reaching the end of the disk. LOOK aims to reduce the average seek time compared to SCAN by avoiding unnecessary scanning to the ends of the disk.</p>\n    <img src=\"https://static.takeuforward.org/content/-NGcS9wFX\" alt=\"LOOK Illustration\">\n    <p><strong>Example:</strong> Using the same example, if the disk arm is at track 53 and the requests are to access tracks 98, 183, 37, 122, and 14, LOOK would first service requests in the direction of movement (e.g., tracks 98, 122), then reverse direction and service requests in the other direction (e.g., tracks 37, 14), stopping before reaching the ends of the disk.</p>\n    <p><strong>Advantages:</strong> LOOK can reduce the average seek time compared to SCAN by avoiding unnecessary scanning to the ends of the disk.</p>\n    <p><strong>Disadvantages:</strong> LOOK may not be optimal for systems with high variance in request distribution, as requests at the ends of the disk may experience longer wait times.</p>\n\n    <h3>C-LOOK Algorithm</h3>\n    <p>The C-LOOK algorithm is a variant of LOOK that only scans in one direction, servicing requests until it reaches the last request in that direction before jumping back to the beginning of the disk. C-LOOK aims to reduce the variance in service times compared to LOOK by ensuring that all requests are serviced in the same direction.</p>\n    <img src=\"https://static.takeuforward.org/content/-roUQsmEf\" alt=\"C-LOOK Illustration\">\n    <p><strong>Example:</strong> Using the same example, if the disk arm is at track 53 and the requests are to access tracks 98, 183, 37, 122, and 14, C-LOOK would first service requests in the direction of movement (e.g., tracks 98, 122), then jump back to the beginning of the disk and continue servicing requests in the same direction (e.g., tracks 14, 37).</p>\n    <p><strong>Advantages:</strong> C-LOOK can reduce the variance in service times compared to LOOK by ensuring that all requests are serviced in the same direction.</p>\n    <p><strong>Disadvantages:</strong> C-LOOK may lead to increased average seek times compared to LOOK, especially if there are requests at both ends of the disk.</p>\n\n    <h3>Various Disk Scheduling Policies</h3>\n    <table border=\"1\">\n        <thead>\n            <tr>\n                <th>Algorithm</th>\n                <th>Description</th>\n                <th>Advantages</th>\n                <th>Disadvantages</th>\n            </tr>\n        </thead>\n        <tbody>\n            <tr>\n                <td>First-Come-First-Served</td>\n                <td>Services requests in the order they arrive.</td>\n                <td>Simple implementation ensures fairness.</td>\n                <td>May result in high average seek times.</td>\n            </tr>\n            <tr>\n                <td>Shortest Seek Time First</td>\n                <td>Selects the request with the shortest seek time from the current position of the disk arm.</td>\n                <td>Minimises average seek time.</td>\n                <td>May suffer from starvation.</td>\n            </tr>\n            <tr>\n                <td>SCAN</td>\n                <td>Moves the disk arm in one direction across the disk, servicing requests along the way, then reverses direction and scans back.</td>\n                <td>Provides a good balance between fairness and performance.</td>\n                <td>May not be optimal for systems with high variance in request distribution.</td>\n            </tr>\n            <tr>\n                <td>C-SCAN</td>\n                <td>Scans in one direction until it reaches the end of the disk, then jumps back to the beginning.</td>\n                <td>Reduces variance in service times.</td>\n                <td>May lead to increased average seek times.</td>\n            </tr>\n            <tr>\n                <td>LOOK</td>\n                <td>Services requests only until it reaches the last request in the current direction, then reverses direction.</td>\n                <td>Reduces average seek time compared to SCAN.</td>\n                <td>Requests at the ends of the disk may experience longer wait times.</td>\n            </tr>\n            <tr>\n                <td>C-LOOK</td>\n                <td>Scans in one direction until it reaches the last request, then jumps back to the beginning.</td>\n                <td>Reduces variance in service times.</td>\n                <td>May lead to increased average seek times.</td>\n            </tr>\n        </tbody>\n    </table>\n\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Cache",
        "sl_no_in_step": 11,
        "title": "Cache",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/cache",
        "content": "<p>\n        A cache is a high-speed memory component used to store frequently accessed data and instructions to improve system performance by reducing the latency of memory accesses. Caches are placed between the CPU and main memory (RAM) and exploit the principle of locality, which states that programs tend to access a relatively small portion of memory frequently or exhibit spatial and temporal locality.\n    </p>\n    <img src=\"https://static.takeuforward.org/content/-ico5rxtu\" alt=\"Cache Illustration\">\n    <p>\n        By storing copies of frequently accessed data in a smaller, faster cache memory, the CPU can retrieve data more quickly, thereby reducing the average memory access time.\n    </p>\n\n    <h2>Cache Organisation: Direct-Mapped, Set-Associative, Fully-Associative</h2>\n\n    <h3>Direct-Mapped Cache</h3>\n    <p>\n        In a direct-mapped cache, each main memory address is mapped to a specific location in the cache using a hash function or modulo arithmetic. This mapping ensures that each block of main memory can only reside in one specific cache location. Direct-mapped caches are simple and require minimal hardware, but they may suffer from conflicts where multiple main memory blocks map to the same cache location, leading to cache thrashing.\n    </p>\n    <img src=\"https://static.takeuforward.org/content/-_Riol_QH\" alt=\"Direct-Mapped Cache Illustration\">\n\n    <h3>Set-Associative Cache</h3>\n    <p>\n        In a set-associative cache, each main memory address is mapped to a set of cache locations, and the data can be stored in any location within the set. This organisation reduces the likelihood of conflicts compared to direct-mapped caches while maintaining simplicity and low hardware overhead. Set-associative caches strike a balance between simplicity and flexibility, making them widely used in modern CPU designs.\n    </p>\n\n    <h3>Fully-Associative Cache</h3>\n    <p>\n        In a fully-associative cache, each main memory address can be stored in any cache location, without any restrictions on mapping. This organisation offers the highest degree of flexibility and minimises the potential for conflicts, but it requires complex hardware for tag comparison and cache management, leading to higher power consumption and cost.\n    </p>\n\n    <h2>Cache Coherence</h2>\n    <p>\n        Cache coherence refers to the consistency of data stored in multiple caches that reference the same memory location. In multiprocessor systems, each processor typically has its own cache memory to improve performance. However, when multiple processors access and modify the same memory location concurrently, ensuring cache coherence becomes crucial to prevent data inconsistencies.\n    </p>\n    <img src=\"https://static.takeuforward.org/content/-Chkg7AZG\" alt=\"Cache Coherence Illustration\">\n    <p>\n        Cache coherence protocols, such as MESI (Modified, Exclusive, Shared, Invalid) or MOESI (Modified, Owned, Exclusive, Shared, Invalid), are used to maintain coherence by coordinating cache operations, invalidating or updating cache lines as needed, and ensuring that all processors observe a consistent view of memory.\n    </p>\n\n    <h2>Cache Replacement Policies</h2>\n    <p>Cache replacement policies determine which cache line to evict when the cache is full and a new line needs to be loaded. Common cache replacement policies include:</p>\n\n    <h3>Least Recently Used (LRU)</h3>\n    <p>\n        LRU replaces the cache line that has not been accessed for the longest time. This policy aims to minimise the likelihood of evicting recently accessed data, assuming that recently accessed data is more likely to be accessed again soon.\n    </p>\n\n    <h3>First-In, First-Out (FIFO)</h3>\n    <p>\n        FIFO replaces the cache line that was loaded into the cache earliest. This policy is simple to implement but may not always reflect access patterns accurately, especially in scenarios with non-uniform memory access patterns.\n    </p>\n\n    <h3>Random Replacement</h3>\n    <p>\n        Random replacement selects a cache line to evict randomly from the set of available cache lines. While simple to implement and not prone to algorithmic complexity, random replacement may result in suboptimal performance compared to more sophisticated policies like LRU.\n    </p>\n\n    <h3>Least Frequently Used (LFU)</h3>\n    <p>\n        LFU replaces the cache line that has been accessed the fewest times. This policy aims to evict cache lines that are least frequently accessed, assuming that they are less likely to be accessed in the future. However, accurately tracking access frequency can be challenging and may require additional hardware support.\n    </p>\n\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Direct-and-Associative-mapping",
        "sl_no_in_step": 12,
        "title": "Direct and Associative mapping",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/direct-associative-mapping",
        "content": "    <h3>Direct Mapping</h3>\n    <h3>Concept of Direct Mapping</h3>\n    <p>\n        Direct mapping is a cache mapping technique where each block of main memory is mapped to exactly one location in the cache using a simple modulo operation. The address of a memory block is divided into three fields: the tag, index, and block offset. The index field identifies the specific cache line, while the tag field helps to verify if the data in the indexed cache line corresponds to the requested memory block.\n    </p>\n<img src=\"https://static.takeuforward.org/content/-_Riol_QH\">\n    <p>\n        This mapping method is straightforward to implement due to its simplicity, making it suitable for hardware implementations that prioritize speed. However, direct mapping may lead to frequent cache misses in cases where multiple memory blocks map to the same cache line, a situation known as cache thrashing.\n    </p>\n\n    <h3>Implementation of Direct Mapping</h3>\n    <p>\n        In direct mapping, each memory block is placed in a cache line determined by the formula:\n    </p>\n    <p>\n        <code>Cache Line Number = (Memory Block Address) % (Number of Cache Lines)</code>\n    </p>\n    <p>\n        For example, if there are 16 cache lines, and the memory block address is 18, the block will be stored in the cache line number 18 % 16 = 2. The tag field is then used to ensure that the data in cache line 2 corresponds to the requested memory block during access.\n    </p>\n<img src=\"https://static.takeuforward.org/content/-M8zeEMhV\", width=400>\n\n    <h3>Associative Mapping</h3>\n    <h3>Concept of Associative Mapping</h3>\n    <p>\n        Associative mapping is a more flexible cache mapping technique where a memory block can be stored in any cache line rather than being restricted to a specific one. This is achieved by using a tag field that identifies which memory block is currently stored in each cache line. During access, the cache searches all lines in parallel for the tag that matches the requested memory address.\n    </p>\n    <p>\n        While this technique offers more flexibility and reduces the risk of cache thrashing, it requires more complex hardware to compare tags across all cache lines simultaneously. The increased hardware complexity can lead to higher costs and power consumption, making associative mapping more suitable for smaller caches.\n    </p>\n<img src=\"https://static.takeuforward.org/content/-jeVtPUvy\", width=400>\n    <h3>Implementation of Associative Mapping</h3>\n    <p>\n        In fully associative mapping, any memory block can be placed in any cache line. During data retrieval, the cache checks each line for a matching tag, which indicates that the desired data is present. The implementation relies on a process called *tag comparison*, where all cache lines are searched in parallel for a match.\n    </p>\n    <p>\n        For example, if a memory block with address 25 is requested, the cache will search through all lines to see if any contain a tag that matches the address. If a match is found, the corresponding data is retrieved; otherwise, the memory block is loaded into an available cache line, possibly replacing an existing one based on the cache replacement policy.\n    </p>\n\n    <h3>Comparison Between Direct and Associative Mapping</h3>\n    <table border=\"1\" cellpadding=\"10\" cellspacing=\"0\">\n        <thead>\n            <tr>\n                <th>Aspect</th>\n                <th>Direct Mapping</th>\n                <th>Associative Mapping</th>\n            </tr>\n        </thead>\n        <tbody>\n            <tr>\n                <td><strong>Simplicity</strong></td>\n                <td>Simpler to implement with a straightforward mapping function. Requires less hardware.</td>\n                <td>More complex due to the need for tag comparison across all cache lines.</td>\n            </tr>\n            <tr>\n                <td><strong>Flexibility</strong></td>\n                <td>Each memory block is mapped to a specific cache line, increasing chances of conflicts.</td>\n                <td>Offers greater flexibility since any memory block can be stored in any cache line.</td>\n            </tr>\n            <tr>\n                <td><strong>Performance</strong></td>\n                <td>Prone to cache thrashing, which can lead to lower hit rates in some scenarios.</td>\n                <td>Typically higher hit rates due to reduced conflict misses, leading to better performance.</td>\n            </tr>\n            <tr>\n                <td><strong>Cost and Complexity</strong></td>\n                <td>Lower cost and power consumption due to simpler hardware requirements.</td>\n                <td>Higher cost and power consumption due to complex comparison logic.</td>\n            </tr>\n        </tbody>\n    </table>\n\n    <h3>Hybrid Mapping Techniques</h3>\n    <p>\n        Hybrid mapping techniques combine elements of both direct and associative mapping to balance performance and hardware complexity. The most common hybrid technique is the *set-associative cache*, where the cache is divided into multiple sets, and each memory block maps to a specific set, but can be placed in any line within that set.\n    </p>\n    <p>\n        Set-associative mapping is typically implemented as <em>n-way set-associative</em>, where <em>n</em> refers to the number of lines in each set. For example, in a 4-way set-associative cache, each memory block maps to a specific set and can occupy any of the 4 cache lines within that set.\n    </p>\n    <p>\n        This approach offers a middle ground between direct and fully associative caches, reducing the likelihood of conflicts compared to direct mapping while avoiding the high hardware complexity of fully associative caches. Set-associative caches are widely used in modern CPU designs due to their balanced approach.\n    </p>"
      }
    ]
  },
  {
    "step_no": 4,
    "topic": "File Systems",
    "data": [
      {
        "id": "file-system,-and-what-are-its-components",
        "sl_no_in_step": 1,
        "title": "file system, and what are its components",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/file-system",
        "content": "    <p>\n        A file system is a method and data structure that an operating system uses to control how data is stored and retrieved on a storage device.\n    </p>\n    <p>\n        The primary purpose of a file system is to manage the storage of data so that it is easy to find and access. It organises data into files and directories, maintains <b>metadata</b>, and manages the allocation of space on the storage medium.\n    </p>\n    <p>\n        A file system is a crucial component of any operating system, serving as a framework for organising and managing the storage and retrieval of data on storage devices such as hard drives, SSDs, and USB drives. It provides a way to store data in an organised manner using files and directories, making it easy to locate and manipulate.\n    </p>\n    <p>\n        The file system abstracts the physical aspects of the storage medium, allowing users and applications to interact with data using logical structures. This abstraction simplifies data management and access, ensuring that data can be stored efficiently and retrieved quickly when needed. Additionally, file systems manage the allocation of space on the storage device, optimising the use of available storage and preventing conflicts.\n    </p>\n\n    <h3>Hierarchical Structure</h3>\n    <img src=\"https://static.takeuforward.org/content/-wOciOtBZ\" alt=\"Hierarchical Structure\">\n    <p>\n        The hierarchical structure of a file system is akin to a tree, with a root directory at the top that branches out into subdirectories and files. This structure facilitates an organised and intuitive way to store and access data. Each directory can contain multiple files and subdirectories, creating a multi-level organisation that can represent complex data relationships.\n    </p>\n    <p>\n        For example, a user's home directory might contain subdirectories for documents, pictures, and music, each of which can have further subdivisions. This hierarchical approach not only helps in logical organisation but also enhances security by allowing permissions to be set at different levels. The structure ensures that users can navigate and manage files effectively, maintaining an organised system that is easy to understand and use.\n    </p>\n\n    <h3>Metadata</h3>\n    <p>\n        <b>Metadata</b> in a file system provides essential information about each file and directory, enabling the system and users to understand its properties and status. Common metadata elements include:\n    </p>\n    <ul>\n        <li>File name</li>\n        <li>Size</li>\n        <li>Type</li>\n        <li>Creation and modification timestamps</li>\n        <li>Ownership</li>\n        <li>Access permissions</li>\n    </ul>\n    <p>\n        This information is critical for various file system operations.\n    </p>\n    <p>\n        For example, timestamps help in identifying the latest versions of files, while permissions ensure that only authorised users can access or modify certain files. Metadata also plays a role in system performance and security, as it allows the file system to efficiently manage access and maintain integrity. Advanced file systems may include additional metadata such as extended attributes, which can store custom information relevant to specific applications.\n    </p>\n\n    <h3>Access Methods</h3>\n    <p>Access methods define how data within files can be accessed and manipulated. Different methods are suitable for different use cases:</p>\n    <ul>\n        <li><b>Sequential Access:</b> This method reads or writes data in a linear sequence, suitable for tasks where data is processed in order, such as reading a log file.</li>\n        <img src=\"https://static.takeuforward.org/content/-MpYah_Nq\" alt=\"Sequential Access\">\n        <li><b>Direct Access:</b> Allows data to be read or written at any location within the file without sequentially reading preceding data. This is ideal for databases and applications requiring random data access.</li>\n        <li><b>Indexed Access:</b> Utilises an index to quickly locate the data blocks within a file, improving access speed for large files with complex structures.</li>\n        <img src=\"https://static.takeuforward.org/content/-DYRishqN\" alt=\"Indexed Access\">\n        <li><b>Random Access:</b> Similar to direct access but often used for smaller data chunks within a file, enabling quick retrieval or modification of specific sections of the file.</li>\n    </ul>\n    <p>Each method has its own advantages and is chosen based on the specific needs of the application or system.</p>\n\n    <h3>File System Operations</h3>\n    <p>File system operations encompass a range of actions that can be performed on files and directories, enabling users and applications to manage data effectively. These operations include:</p>\n    <ul>\n        <li>Creating Files and Directories: Establishing new storage locations for data.</li>\n        <li>Reading and Writing Data: Accessing and modifying the contents of files.</li>\n        <li>Renaming and Moving: Changing the names or locations of files and directories.</li>\n        <li>Deleting: Removing files or directories from the storage medium.</li>\n        <li>Managing Permissions: Setting access controls to ensure only authorised users can perform certain actions.</li>\n    </ul>\n    <p>These operations are fundamental to the functioning of a file system, allowing users to organise, protect, and interact with their data.</p>\n\n    <h3>File System Integrity</h3>\n    <p>\n        Maintaining <b>file system integrity</b> is crucial to ensuring data consistency and reliability. Various techniques are employed to protect against data corruption, loss, and unauthorised access. These techniques include:\n    </p>\n    <ul>\n        <li><b>Journaling:</b> Keeping a log of changes to the file system, which helps in recovering from crashes or unexpected failures by replaying the logged changes.</li>\n        <li><b>Checksums:</b> Used to verify data integrity by comparing stored and computed values, ensuring that data has not been altered or corrupted.</li>\n        <li><b>File System Consistency Checks (FSCC):</b> Utilities that scan the file system for inconsistencies and repair any detected issues.</li>\n        <li><b>Redundancy Methods:</b> Such as mirroring or using parity bits to protect data against hardware failures by storing multiple copies or using error-correcting codes.</li>\n    </ul>\n    <p>These techniques work together to ensure the file system remains reliable and resilient, safeguarding the data stored within it.</p>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Types-of-file-system",
        "sl_no_in_step": 2,
        "title": "Types of file system",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/types-of-file-system",
        "content": "<h3>Disk-based File Systems</h3>\n<p>Disk-based file systems are the traditional file systems used to manage data on physical storage devices like hard drives, SSDs, and optical discs. These file systems are responsible for organising data into files and directories, managing space on the disk, and ensuring data integrity and security.</p>\n\n<h3>FAT (File Allocation Table)</h3>\n<img src=\"https://static.takeuforward.org/content/-VAESMP3S\" alt=\"FAT File System\" >\n<p>FAT is one of the oldest file systems, known for its simplicity and wide compatibility. It's commonly used in removable media like USB drives and memory cards. FAT has several variants, including FAT12, FAT16, and FAT32, each offering different limits on file and partition sizes. While FAT is simple and supported by virtually all operating systems, it lacks advanced features and scalability, making it unsuitable for modern large-scale storage needs.</p>\n\n<h3>NTFS (New Technology File System)</h3>\n<p>NTFS is a robust and advanced file system used primarily in Windows environments. It offers features like file compression, encryption, access control lists (ACLs) for security, and journaling to maintain file system integrity in case of crashes. NTFS supports large volumes and file sizes, making it suitable for both personal computers and enterprise servers.</p>\n\n<h3>ext4 (Fourth Extended File System)</h3>\n<p>ext4 is a widely-used file system in Linux environments. It improves on its predecessors (ext2 and ext3) by offering larger volume and file size support, extents for better performance, and faster file system checks. ext4 is known for its reliability and efficiency, making it a popular choice for Linux-based systems.</p>\n\n<h3>HFS+ (Hierarchical File System Plus)</h3>\n<img src=\"https://static.takeuforward.org/content/-wOciOtBZ\" alt=\"HFS+\" width=\"400\">\n<p>HFS+ is used in macOS systems. It supports features like journaling, large file sizes, and efficient storage management. HFS+ has been largely replaced by the more advanced APFS (Apple File System) in newer macOS versions, but it remains in use on older systems and some storage devices.</p>\n\n<h3>Network File Systems</h3>\n<p>Network file systems enable the sharing of files across a network, allowing multiple users and systems to access and manage data stored on a central server. These systems are essential for collaborative environments and distributed computing.</p>\n\n<h3>NFS (Network File System)</h3>\n<p>NFS is a distributed file system protocol used primarily in Unix and Linux environments. It allows a computer to access files over a network as if they were on its local disks. NFS supports features like file locking and access control, enabling efficient file sharing and collaboration.</p>\n<img src=\"https://static.takeuforward.org/content/-IQmA5VAU\" alt=\"NFS\" width=\"400\">\n\n<h3>CIFS/SMB (Common Internet File System/Server Message Block)</h3>\n<p>CIFS, also known as SMB, is used primarily in Windows networks for file and printer sharing. It provides robust features like file locking, authentication, and encrypted communications. SMB is widely used in both small office and enterprise environments.</p>\n\n<h3>AFS (Andrew File System)</h3>\n<p>AFS is a distributed file system that offers scalable file sharing across large networks. It provides features like strong security through Kerberos authentication, volume management, and efficient caching. AFS is used in academic and research institutions for large-scale distributed file sharing.</p>\n\n<h3>DFS (Distributed File System)</h3>\n<p>DFS is a set of client and server services in Windows Server that enables the creation of a single namespace for multiple file servers and shares. It simplifies data access and management in large networked environments by providing redundancy and load balancing.</p>\n\n<h3>Distributed File Systems</h3>\n<p>Distributed file systems manage data across multiple servers or storage devices, providing high availability, fault tolerance, and scalability. They are essential for handling large-scale data storage and processing in cloud and big data environments.</p>\n\n<h3>HDFS (Hadoop Distributed File System)</h3>\n<p>HDFS is designed for the storage and processing of large datasets in a distributed computing environment. It provides high throughput access to application data and is fault-tolerant, ensuring data replication and reliability across a cluster of machines. HDFS is a cornerstone of the Hadoop ecosystem used in big data analytics.</p>\n\n<h3>GFS (Google File System)</h3>\n<p>GFS, now succeeded by Colossus, is designed to support large-scale data processing at Google. It provides fault tolerance through data replication, efficient data streaming, and scalability to handle massive datasets across thousands of machines. GFS is optimized for large, sequential data reads and writes.</p>\n\n<h3>Ceph</h3>\n<p>Ceph is a distributed storage system that provides object, block, and file storage in a unified system. It is designed for high performance, scalability, and reliability, using intelligent data placement and replication to ensure fault tolerance and data integrity. Ceph is widely used in cloud storage solutions and enterprise environments.</p>\n\n<h3>GlusterFS</h3>\n<p>GlusterFS is a scalable network file system suitable for data-intensive tasks such as cloud storage and media streaming. It aggregates storage resources from multiple servers into a single namespace, providing high availability and performance. GlusterFS supports features like data replication, striping, and volume management.</p>\n\n<h3>Amazon S3 (Simple Storage Service)</h3>\n<p>S3 is an object storage service provided by Amazon Web Services (AWS). It offers scalable, high-durability storage for any type of data, accessible over the web via APIs. S3 is used for backup, archival, big data analytics, and content distribution.</p>\n\n<h3>Specialized File Systems</h3>\n\n<h3>ZFS (Zettabyte File System)</h3>\n<p>ZFS is designed for data integrity, scalability, and advanced storage management. It includes features like data deduplication, compression, snapshots, and copy-on-write. ZFS ensures data integrity through end-to-end checksumming and self-healing capabilities, making it ideal for enterprise storage and data management solutions.</p>\n\n<h3>Btrfs (B-tree File System)</h3>\n<p>Btrfs is a modern Linux file system designed for fault tolerance, repair, and easy administration. It supports features like snapshots, subvolumes, compression, and integrated multi-device spanning. Btrfs is optimised for high performance and efficient storage management, making it suitable for servers and advanced storage systems.</p>\n\n<h3>F2FS (Flash-Friendly File System)</h3>\n<p>F2FS is optimised for NAND flash-based storage devices such as SSDs and eMMC. It aims to address the specific characteristics of flash memory, providing high performance and longevity. F2FS includes features like garbage collection, wear levelling, and efficient data allocation to optimise flash memory usage.</p>\n\n<h3>ReFS (Resilient File System)</h3>\n<p>ReFS is a Microsoft file system designed for data integrity, scalability, and resilience to data corruption. It supports features like integrity streams, large volume and file size support, and data scrubbing. ReFS is used in scenarios requiring high reliability and data protection, such as enterprise storage solutions.</p>\n\n<h3>Virtual File Systems</h3>\n<p>Virtual file systems provide an abstraction layer that allows different file systems to be accessed in a uniform way, presenting a consistent interface to users and applications regardless of the underlying storage medium.</p>\n\n<h3>procfs (Process File System)</h3>\n<p>procfs is a virtual file system in Unix-like systems that presents process information as files. It allows users and applications to interact with kernel data structures, such as process status and system information, through a standardized file interface. procfs simplifies system monitoring and management by providing a consistent and accessible view of system internals.</p>\n\n<h3>tmpfs (Temporary File System)</h3>\n<p>tmpfs is a virtual file system that stores files in volatile memory (RAM), providing fast access to temporary files that do not need to persist across reboots. tmpfs is used for tasks requiring high-speed storage, such as temporary data for running applications and caching. It combines the speed of RAM with the flexibility of a file system interface.</p>\n\n<h3>FUSE (Filesystem in Userspace)</h3>\n<p>FUSE allows users to create custom file systems without modifying the kernel. It provides a framework for developing file systems that run in user space, enabling the integration of diverse storage solutions and specialized file systems. FUSE is widely used for implementing file systems with unique features or specific application requirements.</p>\n\n<h3>OverlayFS</h3>\n<p>OverlayFS is a virtual file system that allows multiple file systems to be overlaid, presenting a single unified view. It is commonly used in container environments to provide writable layers on top of read-only base images. OverlayFS simplifies file system management in scenarios where changes need to be isolated and layered.</p>\n\n<h3>UnionFS</h3>\n<p>UnionFS is a virtual file system that merges multiple file systems into a single coherent file system. It allows files and directories from"
      },
      {
        "id": "file-allocation-and-deallocation",
        "sl_no_in_step": 3,
        "title": "file allocation and deallocation",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/file-allocation-deallocation",
        "content": "\n<h3>Contiguous Allocation</h3>\n<p>Contiguous allocation is a file allocation method where each file is stored in a single contiguous block of disk space. This means that all the blocks occupied by a file are next to each other on the disk.</p>\n<p>The primary advantage of contiguous allocation is its simplicity and efficiency in accessing files. Since the entire file is stored in a single continuous block, the system can quickly locate and read the file sequentially, leading to high performance for sequential file access. Additionally, calculating the location of any specific part of the file is straightforward.</p>\n<img src=\"https://static.takeuforward.org/content/-hMawFssQ\" alt=\"Contiguous Allocation\" width=\"400\">\n<p>The major drawback of contiguous allocation is the problem of external fragmentation, where free space is scattered in small, non-contiguous blocks. This makes it difficult to find large contiguous spaces for new files, leading to insufficient disk space utilisation. Additionally, resizing files can be problematic, as there may not be enough contiguous space available to accommodate the new size.</p>\n<p>Contiguous allocation is best suited for situations where files are relatively static in size and the storage system does not frequently add or remove files, such as read-only media like CD-ROMs.</p>\n\n<h3>Linked Allocation</h3>\n<p>Linked allocation addresses the fragmentation issue of contiguous allocation by allowing a file to be stored in non-contiguous blocks that are linked together.</p>\n<p>Linked allocation eliminates external fragmentation, as each file block can be placed anywhere on the disk. It also simplifies file resizing, as additional blocks can be easily linked to the existing file chain.</p>\n<img src=\"https://static.takeuforward.org/content/-MpYah_Nq\" alt=\"Linked Allocation\" width=\"400\">\n<p>The major disadvantage is the added overhead of maintaining pointers in each block, which reduces the effective storage capacity. Furthermore, accessing a file sequentially can be slower because the system must read the pointer from each block to locate the next block. Random access is particularly inefficient, as it requires traversing the entire chain of pointers.</p>\n<p>Linked allocation is suitable for systems where files are frequently created, deleted, or resized, such as certain database systems and file storage with many small files.</p>\n\n<h3>Indexed Allocation</h3>\n<p>Indexed allocation combines the benefits of contiguous and linked allocation by using an index block to store pointers to all the blocks of a file.</p>\n<p>Indexed allocation supports efficient random access to file blocks, as the index provides direct pointers to each block. This method reduces external fragmentation and allows files to grow without needing contiguous blocks. Additionally, it simplifies the management of file blocks.</p>\n<img src=\"https://static.takeuforward.org/content/-DYRishqN\" alt=\"Indexed Allocation\" width=\"400\">\n<p>The main disadvantage is the overhead of storing the index block, which consumes additional disk space. For very large files, multiple index blocks might be needed, increasing complexity. Indexed allocation also requires a robust mechanism to handle the index blocks efficiently.</p>\n<p>Indexed allocation is ideal for large files and systems requiring efficient random access, such as operating systems and applications handling large databases or multimedia files.</p>\n\n<h3>File Allocation Tables (FAT)</h3>\n<p>The File Allocation Table (FAT) is a data structure used by the FAT file system to manage disk space and keep track of the allocation status of each block.</p>\n<p>FAT is simple and widely supported across different operating systems, making it highly compatible for use in removable media like USB drives and memory cards. The table structure allows easy navigation and management of file blocks.</p>\n<img src=\"https://static.takeuforward.org/content/-VAESMP3S\" alt=\"FAT\" width=\"400\">\n<p>FAT can suffer from fragmentation, both internal and external, leading to insufficient disk space usage. It also becomes less efficient as the disk size increases, because the table must be read frequently, which can slow down access times. Additionally, the simplicity of FAT means it lacks advanced features found in modern file systems, such as security and journaling.</p>\n<p>FAT is commonly used for removable storage devices and systems requiring wide compatibility, such as cameras, MP3 players, and other consumer electronics.</p>\n\n<h3>Allocation Methods Comparison</h3>\n<table border=\"1\">\n    <thead>\n        <tr>\n            <th>Aspect</th>\n            <th>Contiguous Allocation</th>\n            <th>Linked Allocation</th>\n            <th>Indexed Allocation</th>\n            <th>FAT</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>Ease of Implementation</td>\n            <td>Simple</td>\n            <td>Moderate</td>\n            <td>Complex</td>\n            <td>Simple</td>\n        </tr>\n        <tr>\n            <td>Sequential Access Performance</td>\n            <td>High</td>\n            <td>Moderate</td>\n            <td>High</td>\n            <td>Moderate</td>\n        </tr>\n        <tr>\n            <td>Random Access Performance</td>\n            <td>Low</td>\n            <td>Low</td>\n            <td>High</td>\n            <td>Moderate</td>\n        </tr>\n        <tr>\n            <td>Fragmentation</td>\n            <td>High</td>\n            <td>Low</td>\n            <td>Low</td>\n            <td>Moderate to High</td>\n        </tr>\n        <tr>\n            <td>Space Utilisation</td>\n            <td>Low (due to Fragmentation)</td>\n            <td>Moderate</td>\n            <td>High</td>\n            <td>Moderate</td>\n        </tr>\n        <tr>\n            <td>Flexibility in File Resizing</td>\n            <td>Low</td>\n            <td>High</td>\n            <td>High</td>\n            <td>Moderate</td>\n        </tr>\n    </tbody>\n</table>\n\n<h3>File Deallocation Strategies</h3>\n<p>File deallocation involves reclaiming space that was previously allocated to files that are no longer needed.</p>\n\n<h3>Immediate Deallocation</h3>\n<p>This strategy immediately releases the space occupied by a file back to the pool of free space as soon as the file is deleted. It ensures that space is available for new files without delay but can lead to fragmentation if not managed carefully.</p>\n\n<h3>Delayed Deallocation</h3>\n<p>In this approach, space is not immediately reclaimed upon file deletion. Instead, the system marks the file for deletion and reclaims the space during a scheduled maintenance period or when space is critically low. This can reduce fragmentation and improve performance but requires careful management to avoid running out of space.</p>\n\n<h3>Garbage Collection</h3>\n<p>Some file systems, particularly those used in SSDs and flash memory, employ garbage collection to manage deallocation. This involves periodically scanning the storage to identify and reclaim unused space, optimising storage usage and performance.</p>\n\n<h3>Reference Counting</h3>\n<p>This method keeps track of how many references point to a file or block of data. Space is only deallocated when the reference count drops to zero, ensuring that shared data is not prematurely deleted. This is useful in systems with shared files or data blocks, such as versioned file systems.</p>\n\n<h3>Deferred Deletion with Secure Wipe</h3>\n<p>To enhance security, some systems use deferred deletion combined with secure wiping, where the space occupied by a deleted file is overwritten with random data before being released. This prevents recovery of sensitive data from deleted files but can add overhead to the deallocation process.</p>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "fragmentation",
        "sl_no_in_step": 4,
        "title": "fragmentation",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/fragmentation",
        "content": "\n<p>Fragmentation refers to the condition of a storage device where files are not stored in contiguous blocks, resulting in parts of files being scattered across different areas of the disk. This scattered arrangement can lead to inefficient use of storage space and reduced system performance.</p>\n<img src=\"https://static.takeuforward.org/content/-qyHAR_0G\" alt=\"Fragmentation\" >\n<p>Fragmentation is caused by frequent creation, deletion, and resizing of files. When files are deleted, they leave gaps that may not be large enough to accommodate new files, leading to fragmentation. File resizing can also contribute, as the new size might require additional non-contiguous blocks.</p>\n\n<h3>Types of Fragmentation</h3>\n\n<h3>External Fragmentation</h3>\n<p>External Fragmentation occurs when free space is divided into small, non-contiguous blocks scattered throughout the disk. Even though there is enough total free space to store a new file, the lack of contiguous free space blocks prevents the file from being stored as a single unit.</p>\n<img src=\"https://static.takeuforward.org/content/-WPJUfs4t\" alt=\"External Fragmentation\" >\n\n<h3>Internal Fragmentation</h3>\n<p>Internal Fragmentation occurs within allocated space when the allocated blocks are larger than the actual data stored, leaving unused space within the blocks. Fixed block sizes can exacerbate this type of fragmentation.</p>\n<img src=\"https://static.takeuforward.org/content/-O4Zjs0H8\" alt=\"Internal Fragmentation\" >\n\n<h3>Data Fragmentation</h3>\n<p>Data Fragmentation involves files being broken into pieces that are stored in non-contiguous blocks. Linked and indexed allocation methods can lead to data fragmentation, where accessing files requires multiple seek operations.</p>\n<img src=\"https://static.takeuforward.org/content/-vHqPUumz\" alt=\"Data Fragmentation\" >\n\n<h3>Free Space Fragmentation</h3>\n<p>Free Space Fragmentation is the state where free space is available in small, non-contiguous segments. This fragmentation affects the allocation of new files or resizing of existing files, leading to insufficient disk space utilisation.</p>\n\n<h3>File System Fragmentation</h3>\n<p>File System Fragmentation encompasses all types of fragmentation within a file system, including external, internal, data, and free space fragmentation. It reflects the overall inefficiency in file storage and management.</p>\n\n<h3>Impact on Performance</h3>\n<p>Fragmentation can significantly degrade system performance in several ways:</p>\n<ul>\n    <li><strong>Slower Read/Write Speeds:</strong> Fragmentation requires the disk head to move more frequently to access different parts of a file, resulting in increased seek times and slower data transfer rates.</li>\n    <li><strong>Increased Disk Wear:</strong> Frequent seeking due to fragmented files causes more mechanical wear on hard drives, potentially shortening their lifespan.</li>\n    <li><strong>Higher CPU Usage:</strong> Fragmentation increases the workload on the CPU as it has to manage and locate fragmented file pieces, leading to higher system resource consumption.</li>\n    <li><strong>Inefficient Storage Utilisation:</strong> Fragmentation leads to inefficient use of disk space, as it becomes difficult to find contiguous free space for new files, leading to wasted storage capacity.</li>\n    <li><strong>System Slowdowns:</strong> Overall system performance can suffer due to the cumulative effects of fragmentation, affecting everything from boot times to application load times and general responsiveness.</li>\n</ul>\n\n<h3>Fragmentation Prevention</h3>\n<p>Preventing fragmentation involves strategies to minimise its occurrence:</p>\n<ul>\n    <li><strong>Efficient Allocation Methods:</strong> Using allocation methods that minimise fragmentation, such as indexed allocation, can help. These methods allow files to grow without needing contiguous space.</li>\n    <li><strong>Regular Maintenance:</strong> Scheduling regular defragmentation tasks can help keep fragmentation under control. This process reorganises files and free space to optimise storage layout.</li>\n    <li><strong>Dynamic Block Sizes:</strong> Using file systems that support dynamic block sizes can reduce internal fragmentation by better matching block sizes to file sizes.</li>\n    <li><strong>File System Design:</strong> Choosing file systems that are designed to handle fragmentation efficiently, such as ext4 or NTFS, can help mitigate the impact.</li>\n    <li><strong>Avoiding Frequent Resizing:</strong> Minimising the frequent resizing of files or using file systems that handle resizing efficiently can prevent fragmentation from excessive file growth and shrinking.</li>\n</ul>\n\n<h3>Fragmentation Management</h3>\n<p>Managing fragmentation involves both reactive and proactive strategies to maintain optimal performance:</p>\n<ul>\n    <li><strong>Defragmentation Tools:</strong> Using defragmentation tools that rearrange fragmented files and consolidate free space. Most operating systems provide built-in tools for this purpose.</li>\n    <li><strong>Garbage Collection:</strong> Implementing garbage collection in file systems, particularly those used in SSDs, to regularly clean up fragmented and unused space.</li>\n    <li><strong>Monitoring and Alerts:</strong> Using monitoring tools to track fragmentation levels and set alerts for when fragmentation reaches a threshold requiring action.</li>\n    <li><strong>File System Upgrades:</strong> Upgrading to file systems with better fragmentation handling can significantly improve performance and reduce maintenance overhead.</li>\n    <li><strong>Disk Management Policies:</strong> Implementing disk management policies that include regular maintenance schedules, usage patterns that minimise fragmentation, and training users on best practices for file management.</li>\n</ul>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      }
    ]
  },
  {
    "step_no": 5,
    "topic": "I/O Systems",
    "data": [
      {
        "id": "blocking-vs-non-blocking",
        "sl_no_in_step": 1,
        "title": "blocking vs. non-blocking",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/blocking-non-blocking",
        "content": "    <p>Blocking and non-blocking are terms used to describe the behavior of operations in computing, particularly in I/O operations and communication between processes or threads.</p>\n    <p>A <b>blocking operation</b> is one that does not return control to the caller until it has completed. For example, a blocking read operation on a file or network socket will wait until data is available before returning.</p>\n    <p>A <b>non-blocking operation</b>, on the other hand, returns immediately, without waiting for the operation to complete. If the operation cannot be completed immediately, the system will return an indication that it would have been blocked.</p>\n\n    <h3>Synchronous Blocking</h3>\n    <p>Synchronous blocking operations wait for the completion of a task before proceeding to the next one. During this wait, the calling process or thread is effectively halted.</p>\n    <p><b>Example:</b> Reading from a file where the process waits until the data is read before continuing with the next instruction.</p>\n    <p><b>Use Cases:</b> Simple applications where the simplicity of code is more critical than performance. Suitable for tasks where waiting is acceptable and does not impact overall system performance significantly, such as command-line utilities.</p>\n    <img src=\"https://static.takeuforward.org/content/-YIZKFq-i\" alt=\"Synchronous Blocking Example\">\n\n    <h3>Synchronous Non-Blocking</h3>\n    <p>Synchronous non-blocking operations return control to the caller immediately if the operation would block, allowing the caller to perform other tasks or check back later.</p>\n    <p><b>Example:</b> Attempting to read from a non-blocking socket, which returns immediately if no data is available, letting the program continue other operations.</p>\n    <p><b>Use Cases:</b> Applications that need to maintain responsiveness, like GUI applications or network servers handling multiple connections.</p>\n    <img src=\"https://static.takeuforward.org/content/-coeWHUbj\" alt=\"Synchronous Non-Blocking Example\">\n\n    <h3>Asynchronous Blocking</h3>\n    <p>Asynchronous blocking operations initiate a task and allow the caller to proceed with other operations. However, the caller is eventually blocked when it needs the result of the initiated task.</p>\n    <p><b>Example:</b> Initiating a file read operation and continuing with other tasks, but blocking later when the data is needed and the read operation is not yet complete.</p>\n    <p><b>Use Cases:</b> Scenarios where tasks can be initiated and performed concurrently, but the final results are needed for further processing, like in some multi-threaded applications.</p>\n\n    <h3>Asynchronous Non-Blocking</h3>\n    <p>Asynchronous non-blocking operations allow tasks to be initiated and completed without ever blocking the caller. Completion is usually handled via callbacks, events, or polling.</p>\n    <p><b>Example:</b> Using an asynchronous API to read from a file or a network socket, where a callback function is invoked once the read operation completes.</p>\n    <p><b>Use Cases:</b> High-performance servers, real-time systems, and applications where maintaining maximum responsiveness and concurrency is crucial, such as web servers or user interface applications.</p>\n    <img src=\"https://static.takeuforward.org/content/-SnjOR9uS\" alt=\"Asynchronous Non-Blocking Example\">\n\n    <h3>Performance Comparison</h3>\n    <table>\n        <tr>\n            <th>Criteria</th>\n            <th>Blocking Operations</th>\n            <th>Non-Blocking Operations</th>\n            <th>Synchronous Operations</th>\n            <th>Asynchronous Operations</th>\n        </tr>\n        <tr>\n            <td>Implementation</td>\n            <td>Simple to implement and understand.</td>\n            <td>More complex to implement due to handling partial completions and state management.</td>\n            <td>Easier to reason about as operations occur in a predictable order.</td>\n            <td>More complex due to managing asynchronous control flow, callbacks, and potential race conditions.</td>\n        </tr>\n        <tr>\n            <td>CPU Utilisation</td>\n            <td>Less efficient, can leave the CPU idle during I/O operations.</td>\n            <td>Can achieve higher CPU utilisation and better performance in I/O-bound applications.</td>\n            <td>Can lead to poor performance in I/O-bound or high-latency tasks due to waiting for each operation to complete.</td>\n            <td>Allows multiple tasks to proceed concurrently, leading to better resource utilisation and performance.</td>\n        </tr>\n        <tr>\n            <td>Latency and Responsiveness</td>\n            <td>Suitable for tasks where latency and responsiveness are not critical.</td>\n            <td>Suitable for applications that require high responsiveness and concurrency.</td>\n            <td>Operations are easier to understand but can suffer from high latency in I/O-bound or high-latency environments.</td>\n            <td>Ideal for applications requiring high throughput and responsiveness, such as web servers and real-time systems.</td>\n        </tr>\n        <tr>\n            <td>Use Case Complexity</td>\n            <td>Simplifies coding but not suitable for high-performance scenarios.</td>\n            <td>Requires more sophisticated handling of concurrent tasks and data states.</td>\n            <td>Suitable for simple, predictable operations but not efficient for high-latency or high-throughput environments.</td>\n            <td>Suitable for complex, high-performance applications requiring efficient handling of multiple tasks concurrently.</td>\n        </tr>\n        <tr>\n            <td>Example Use Case</td>\n            <td>\n                <ul>\n                    <li>Command-line tools and scripts.</li>\n                    <li>Simple desktop applications with minimal I/O.</li>\n                </ul>\n            </td>\n            <td>\n                <ul>\n                    <li>Network applications that need to maintain responsiveness.</li>\n                    <li>GUI applications where user interactions should not be blocked by I/O operations.</li>\n                </ul>\n            </td>\n            <td>\n                <ul>\n                    <li>Background tasks in desktop applications.</li>\n                    <li>Scenarios where multiple operations can proceed concurrently, but results are needed for further processing.</li>\n                </ul>\n            </td>\n            <td>\n                <ul>\n                    <li>High-performance web servers (e.g., Node.js).</li>\n                    <li>Real-time systems and applications requiring minimal latency.</li>\n                    <li>Applications with significant I/O operations, like database servers and network services.</li>\n                </ul>\n            </td>\n        </tr>\n    </table>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "synchronous-vs.-asynchronous",
        "sl_no_in_step": 2,
        "title": "synchronous vs. asynchronous",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/syncronous-asyncronous",
        "content": "   <p><b>Synchronous communication</b> refers to a type of operation where tasks are performed sequentially, and each task must be completed before the next one starts. In this mode, the system waits for an operation to complete before moving on to the next operation. This approach ensures a predictable and orderly execution flow but can lead to inefficiencies, especially in I/O-bound tasks, where the CPU may be idle while waiting for operations to complete.</p>\n    \n    <p><b>Asynchronous communication</b> allows tasks to be performed concurrently. Operations can be initiated and then processed in the background while the system moves on to other tasks. This approach does not wait for each task to complete before starting the next one, leading to better resource utilization and responsiveness. Asynchronous communication is more complex to implement due to the need to handle tasks that can complete out of order and manage callbacks or events.</p>\n    \n    <h3>Synchronous Communication</h3>\n    <img src=\"https://static.takeuforward.org/content/-YIZKFq-i\" alt=\"Synchronous Communication Example\">\n    \n    <ul>\n        <li><b>Sequential Execution:</b> Operations are executed one after another. Each task must be completed before the next one starts, making the execution flow predictable and easy to understand.</li>\n        <li><b>Resource Utilisation:</b> Can lead to inefficient use of system resources, as the CPU might be idle while waiting for I/O operations to complete.</li>\n        <li><b>Latency:</b> Higher latency in I/O-bound or high-latency tasks due to waiting for each operation to complete.</li>\n        <li><b>Simplicity:</b> Easier to implement and debug because of the straightforward execution flow.</li>\n        <li><b>Use Cases:</b> Suitable for simple applications, batch processing, and scenarios where the simplicity of the code is more critical than performance.</li>\n    </ul>\n\n    <h3>Asynchronous Communication</h3>\n    <img src=\"https://static.takeuforward.org/content/-SnjOR9uS\" alt=\"Asynchronous Communication Example\">\n    \n    <ul>\n        <li><b>Concurrent Execution:</b> Tasks can be performed concurrently. Operations are initiated and then processed in the background, allowing the system to perform other tasks in parallel.</li>\n        <li><b>Resource Utilisation:</b> More efficient use of system resources, leading to better performance and responsiveness.</li>\n        <li><b>Latency:</b> Lower latency in I/O-bound or high-latency tasks due to the ability to initiate multiple operations simultaneously and continue processing other tasks.</li>\n        <li><b>Complexity:</b> More complex to implement and debug due to the need to manage asynchronous control flow, callbacks, and potential race conditions.</li>\n        <li><b>Use Cases:</b> Ideal for high-performance applications, real-time systems, web servers, and scenarios requiring high throughput and minimal latency.</li>\n    </ul>\n\n    <h3>Blocking vs Non-Blocking in Synchronous Communication</h3>\n    <img src=\"https://static.takeuforward.org/content/-coeWHUbj\" alt=\"Blocking vs Non-Blocking in Synchronous Communication Example\">\n    \n    <table>\n        <tr>\n            <th>Criteria</th>\n            <th>Blocking in Synchronous Communication</th>\n            <th>Non-Blocking in Synchronous Communication</th>\n        </tr>\n        <tr>\n            <td><b>Definition</b></td>\n            <td>The process or thread waits (blocks) for an operation to complete before moving on to the next one.</td>\n            <td>The process or thread does not wait for an operation to complete. Instead, it checks periodically or uses other mechanisms to proceed with other tasks.</td>\n        </tr>\n        <tr>\n            <td><b>Impact on Performance</b></td>\n            <td>Can lead to idle waiting and underutilization of CPU resources, especially in I/O-bound tasks.</td>\n            <td>Better CPU utilisation but can be more complex to manage due to the need to handle partial completions and state management.</td>\n        </tr>\n        <tr>\n            <td><b>Simplicity</b></td>\n            <td>Easier to implement because of the straightforward sequential execution.</td>\n            <td>More complex than blocking operations but still follows a sequential execution flow.</td>\n        </tr>\n        <tr>\n            <td><b>Latency</b></td>\n            <td>Higher latency due to waiting for each operation to complete.</td>\n            <td>Lower latency compared to blocking operations, as the system can continue processing other tasks.</td>\n        </tr>\n        <tr>\n            <td><b>Example</b></td>\n            <td>Reading a file where the process waits until the entire file is read before continuing.</td>\n            <td>Attempting to read from a non-blocking socket, which returns immediately if no data is available, allowing the program to perform other operations.</td>\n        </tr>\n    </table>\n\n    <h3>Blocking vs Non-Blocking in Asynchronous Communication</h3>\n    \n    <table>\n        <tr>\n            <th>Criteria</th>\n            <th>Blocking in Asynchronous Communication</th>\n            <th>Non-Blocking in Asynchronous Communication</th>\n        </tr>\n        <tr>\n            <td><b>Definition</b></td>\n            <td>The operation is initiated asynchronously, but the process or thread eventually blocks when it needs the result of the initiated task.</td>\n            <td>Operations are initiated and processed asynchronously, and the process or thread never blocks, instead using callbacks or events to handle completions.</td>\n        </tr>\n        <tr>\n            <td><b>Impact on Performance</b></td>\n            <td>Can improve overall performance by initiating tasks concurrently but may still suffer from some blocking delays.</td>\n            <td>Maximises performance and resource utilisation by handling multiple operations concurrently without blocking.</td>\n        </tr>\n        <tr>\n            <td><b>Simplicity</b></td>\n            <td>More complex than synchronous blocking but less efficient than fully asynchronous non-blocking operations.</td>\n            <td>Most complex to implement and debug due to the need to manage asynchronous control flow, callbacks, and potential race conditions.</td>\n        </tr>\n        <tr>\n            <td><b>Latency</b></td>\n            <td>Improved compared to synchronous blocking but not as efficient as non-blocking asynchronous operations.</td>\n            <td>Lowest latency and highest efficiency due to continuous processing without waiting for individual tasks to complete.</td>\n        </tr>\n        <tr>\n            <td><b>Example</b></td>\n            <td>Initiating a network request and continuing with other tasks, but blocking when the response is needed.</td>\n            <td>Using an asynchronous API to read from a network socket, where a callback function is invoked once the read operation completes.</td>\n        </tr>\n    </table>\n\n    <h3>Real-World Applications and Examples</h3>\n\n    <ul>\n        <li><b>Synchronous Blocking:</b>\n            <ul>\n                <li>Command-Line Tools: Simple utilities like cp (copy) or ls (list) that perform a sequence of operations and wait for each to complete.</li>\n                <li>Batch Processing: Scripts or applications that process files one at a time, ensuring that each file is fully processed before moving on to the next.</li>\n            </ul>\n        </li>\n        <li><b>Synchronous Non-Blocking:</b>\n            <ul>\n                <li>Network Servers: Servers that need to maintain responsiveness by checking the status of non-blocking socket operations periodically.</li>\n                <li>GUI Applications: Applications that need to perform background tasks without freezing the user interface, such as checking for updates while allowing user interactions.</li>\n            </ul>\n        </li>\n        <li><b>Asynchronous Blocking:</b>\n            <ul>\n                <li>Background Tasks in Desktop Applications: Initiating tasks like file downloads or data processing in the background, but blocking when the results are needed.</li>\n                <li>Concurrent Processing: Applications that initiate multiple I/O operations concurrently but may need to block to wait for certain critical results.</li>\n            </ul>\n        </li>\n        <li><b>Asynchronous Non-Blocking:</b>\n            <ul>\n                <li>High-Performance Web Servers: Servers like Node.js that handle thousands of concurrent connections by using non-blocking I/O and asynchronous operations.</li>\n                <li>Real-Time Systems: Systems that require minimal latency, such as trading platforms or real-time analytics, where operations are processed as they complete without blocking.</li>\n                <li>Large-Scale Data Processing: Applications that process large volumes of data concurrently, such as big data platforms and distributed databases, using asynchronous non-blocking operations for maximum throughput and efficiency.</li>\n            </ul>\n        </li>\n    </ul>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      },
      {
        "id": "Spooling",
        "sl_no_in_step": 3,
        "title": "Spooling",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/spooling",
        "content": "<p><b>Spooling</b> (Simultaneous Peripheral Operations On-Line) is a method used in computing to manage data by placing it in a temporary storage area (usually a buffer or spool) to be processed at a later time. This technique helps in handling I/O operations efficiently, especially when dealing with slower peripheral devices like printers, disks, or network connections.</p>\n\n    <p>Spooling is designed to manage the execution of jobs in a queued manner, allowing for efficient scheduling and processing. It enables a system to continue performing other tasks while waiting for I/O operations to complete.</p>\n    \n    <img src=\"https://static.takeuforward.org/content/-sF1465bd\" alt=\"Spooling Diagram\", width=400>\n\n    <p>Data is temporarily stored in a buffer or a dedicated spool file. This data is then retrieved and processed sequentially by the appropriate device or application. Originally developed to handle batch processing in mainframe computers, spooling has evolved to become a critical component in modern operating systems. It typically involves a spooler, which manages the queue, and spooled devices, which process the queued data. Common examples include:</p>\n    \n    <ul>\n        <li><b>Print Spooling:</b> Refers to the process of queuing print jobs to be processed by a printer. This allows users to submit multiple print jobs without waiting for each one to complete before starting the next.</li>\n        <li><b>Disk Spooling:</b> Involves temporarily storing data on a disk before it is transferred to its final destination, often used in data-intensive applications.</li>\n    </ul>\n\n    <h3>Print Spooling</h3>\n    <p>A print spooler accepts print jobs from applications and stores them in a buffer. The printer then processes these jobs in the order they were received, enhancing user productivity by allowing them to continue working while print jobs are being processed in the background.</p>\n\n    <h3>Disk Spooling</h3>\n    <p>Disk spooling improves system performance by smoothing out the demand on disk resources, reducing bottlenecks, and enhancing overall throughput. It is common in environments with large volumes of data transfer, such as:</p>\n    <ul>\n        <li>Database management systems</li>\n        <li>Video rendering</li>\n        <li>Large-scale data processing</li>\n    </ul>\n    <p>Video editing software often uses disk spooling to manage temporary files during rendering or transcoding processes.</p>\n\n    <h3>Spooling Advantages and Disadvantages</h3>\n    <h4>Advantages:</h4>\n    <ul>\n        <li>Increases system efficiency by allowing other processes to continue while waiting for I/O operations to complete.</li>\n        <li>Optimises the use of peripheral devices by managing queues and scheduling jobs effectively.</li>\n        <li>Provides robust error handling mechanisms, retrying jobs or notifying users in case of issues.</li>\n        <li>Enhances user productivity by allowing them to perform other tasks while waiting for jobs to complete.</li>\n        <li>Facilitates the handling of multiple tasks concurrently, making it suitable for environments with high I/O demands.</li>\n    </ul>\n\n    <h4>Disadvantages:</h4>\n    <ul>\n        <li>Adds complexity to the system, requiring additional components like spoolers and buffers.</li>\n        <li>Consumes additional resources, such as disk space and memory, to manage the spool.</li>\n        <li>May introduce delays in processing if the spooler becomes a bottleneck.</li>\n        <li>Requires careful management and configuration to ensure optimal performance and avoid issues like buffer overflows.</li>\n        <li>System performance can be heavily dependent on the efficiency of the spooling mechanism.</li>\n    </ul>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>"
      }
    ]
  },
  {
    "step_no": 6,
    "topic": "Storage management and data protection",
    "data": [
      {
        "id": "RAID-and-its-types",
        "sl_no_in_step": 1,
        "title": "RAID and its types",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/raid-and-its-types",
        "content": "    <p><b>RAID</b> (Redundant Array of Independent Disks) is a data storage technology that combines multiple physical disk drives into a single logical unit to achieve redundancy, improve performance, or both. Initially, RAID stood for \"Redundant Array of Inexpensive Disks,\" reflecting its use of cost-effective drives to enhance performance and reliability. Today, it is commonly referred to as \"Redundant Array of Independent Disks\" to emphasize the independence of the drives.</p>\n\n    <p>RAID systems are extensively used in servers, workstations, and storage systems to enhance data reliability and access speed. By distributing data across multiple disks, RAID can provide various levels of redundancy and performance improvements, depending on the configuration.</p>\n\n    <p>Different RAID levels offer varying balances between performance, data redundancy, and storage capacity. Each level is designed to address specific needs, such as high-speed data access, fault tolerance, or a combination of both. Understanding the nuances of each RAID level is crucial for selecting the right configuration for a given application, ensuring optimal performance and data protection.</p>\n\n    <h4>RAID 0 (Striping)</h4>\n    <img src=\"https://static.takeuforward.org/content/-nlngmuCH\" alt=\"RAID 0 Diagram\">\n    <p>RAID 0 uses a technique known as striping, where data is divided into blocks and each block is written to a separate disk drive. This method significantly enhances data throughput and improves performance because read and write operations are spread across multiple disks, allowing for parallel access. The striping process effectively multiplies the available bandwidth, making RAID 0 suitable for applications that require high data transfer rates, such as video editing and gaming.</p>\n\n    <p>However, RAID 0 lacks redundancy, which is a significant drawback. If one disk in the array fails, all data stored across the disks is lost, making RAID 0 unsuitable for critical data storage. Despite its performance benefits, the absence of fault tolerance means that RAID 0 should only be used for non-critical applications where speed is the primary concern and data loss can be tolerated.</p>\n\n    <h4>RAID 1 (Mirroring)</h4>\n    <img src=\"https://static.takeuforward.org/content/-nlMGggN2\" alt=\"RAID 1 Diagram\">\n    <p>RAID 1, or mirroring, involves writing identical copies of data to two or more disks. This setup ensures data redundancy, as each disk contains an exact copy of the data. The primary advantage of RAID 1 is its fault tolerance; if one disk fails, the system can still function using the mirrored copy on the other disk. This high level of data protection makes RAID 1 ideal for storing critical data that must be readily available at all times, such as databases and financial records.</p>\n\n    <p>In terms of performance, RAID 1 offers improved read performance since data can be read from any of the mirrored disks. However, write performance remains similar to that of a single disk, as data must be written to all mirrored disks simultaneously. The major trade-off with RAID 1 is storage efficiency, as it effectively reduces usable capacity by half due to data duplication. Despite this, the enhanced data reliability and fault tolerance make RAID 1 a preferred choice for many critical applications.</p>\n\n    <h4>RAID 5 (Striping with Parity)</h4>\n    <img src=\"https://static.takeuforward.org/content/-RfU9xF6Y\" alt=\"RAID 5 Diagram\">\n    <p>RAID 5 combines striping with distributed parity, offering a balance between performance and fault tolerance. Data and parity information are striped across multiple disks, with the parity information allowing the system to reconstruct data in the event of a single disk failure. This configuration provides good read performance due to parallel access to data blocks, although write performance is slower compared to RAID 0 due to the overhead of parity calculations.</p>\n\n    <p>RAID 5 is storage-efficient, providing the total capacity of (N-1) disks, where N is the number of disks in the array. This makes RAID 5 a popular choice for environments that require a balance of good performance, data protection, and efficient use of storage space. Common use cases for RAID 5 include file servers and backup solutions, where the ability to tolerate a single disk failure without data loss is critical.</p>\n\n    <h4>RAID 10 (Striping and Mirroring)</h4>\n    <img src=\"https://static.takeuforward.org/content/-RfU9xF6Y\" alt=\"RAID 10 Diagram\">\n    <p>RAID 10, also known as RAID 1+0, combines the benefits of both RAID 0 and RAID 1 by using both striping and mirroring techniques. Data is striped across multiple disks for high performance and then mirrored for redundancy. This configuration provides high read and write performance due to the striping, while the mirroring ensures data redundancy and fault tolerance.</p>\n\n    <p>Although RAID 10 reduces usable storage capacity by half due to mirroring, it offers the best of both worlds: high performance and high reliability. RAID 10 can sustain multiple disk failures as long as no complete mirror pair fails, making it ideal for applications requiring both high performance and high availability. Typical use cases for RAID 10 include transactional databases, high-performance computing, and any mission-critical applications where downtime is not an option.</p>\n\n    <h3>RAID Performance, Fault Tolerance, and Use Cases</h3>\n    <p>The performance and fault tolerance of RAID levels vary significantly, making it essential to choose the right configuration based on specific needs. RAID 0 offers the highest read/write performance due to striping but lacks fault tolerance, making it suitable only for non-critical applications. RAID 1 provides good read performance and excellent fault tolerance through mirroring, ideal for critical data storage.</p>\n\n    <p>RAID 5 strikes a balance between performance and fault tolerance by using striping with distributed parity, suitable for environments needing efficient storage and reliability. RAID 10, with its combination of striping and mirroring, offers high performance and redundancy, making it perfect for high-demand, mission-critical applications. Each RAID level has its unique strengths and trade-offs, and understanding these is crucial for selecting the appropriate RAID configuration to meet the specific requirements of different applications.</p>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>\n"
      },
      {
        "id": "Security-threats-and-vulnerabilities",
        "sl_no_in_step": 2,
        "title": "Security threats and vulnerabilities",
        "yt_link": null,
        "article_link": "https://takeuforward.org/operating-system/security-threats-vulnerabilities",
        "content": "<p>I/O system security is very important for keeping data safe. It protects the pathways that connect a computer to external devices. These pathways can be vulnerable to many attacks that can affect the <b>integrity</b>, <b>confidentiality</b>, and <b>availability</b> of data.</p>\n    \n    <h3>Data Interception</h3>\n    <p>Data interception happens when unauthorized people access data while it is being sent between devices. This can lead to:</p>\n    <ul>\n        <li>Data breaches</li>\n        <li>Loss of sensitive information</li>\n        <li>Unauthorized disclosure of confidential data</li>\n    </ul>\n    <p>To prevent these risks, encryption methods like SSL/TLS can be used. This means that even if data is intercepted, it cannot be read without the right decryption keys.</p>\n    <img src=\"https://static.takeuforward.org/content/-KAAXaaIY\" alt=\"Data Interception\">\n\n    <h3>Man-in-the-Middle (MitM) Attacks</h3>\n    <p>In a Man-in-the-Middle attack, an attacker secretly intercepts and possibly alters communication between two parties. This can compromise data. For example:</p>\n    <ul>\n        <li>The attacker can listen to conversations (eavesdropping).</li>\n        <li>They can inject harmful data into the communication.</li>\n        <li>They can change the content being sent.</li>\n    </ul>\n    <p>To prevent MitM attacks, strong authentication and encryption should be used.</p>\n    <img src=\"https://static.takeuforward.org/content/-x4UHyud7\" alt=\"Man-in-the-Middle Attacks\">\n\n    <h3>Hardware Tampering</h3>\n    <p>Hardware tampering involves physically changing or compromising I/O devices. For example:</p>\n    <ul>\n        <li>Adding a rogue device that records keystrokes.</li>\n        <li>Modifying firmware to change how a device works.</li>\n        <li>Installing hardware to access data without permission.</li>\n    </ul>\n    <p>To protect against this, organizations should:</p>\n    <ul>\n        <li>Use strong physical security measures.</li>\n        <li>Conduct regular checks on hardware.</li>\n        <li>Use tamper-evident seals to spot unauthorized changes.</li>\n    </ul>\n\n    <h3>Denial of Service (DoS) Attacks</h3>\n    <p>Denial of Service attacks aim to make I/O systems unusable. These attacks can overwhelm a system with too many requests, causing it to:</p>\n    <ul>\n        <li>Slow down</li>\n        <li>Crash</li>\n    </ul>\n    <p>To mitigate DoS attacks, robust network security measures, such as firewalls and intrusion detection systems, should be implemented.</p>\n    <img src=\"https://static.takeuforward.org/content/-Cp7xLdhw\" alt=\"Denial of Service Attacks\">\n\n    <h3>Buffer Overflows</h3>\n    <p>A buffer overflow occurs when a program writes more data to a buffer than it can hold. This can lead to:</p>\n    <ul>\n        <li>Unexpected behavior in the system</li>\n        <li>Executing harmful code</li>\n        <li>Unauthorized access to the system</li>\n    </ul>\n    <p>To prevent buffer overflows, developers should:</p>\n    <ul>\n        <li>Use secure coding practices.</li>\n        <li>Use automated tools to find potential issues.</li>\n        <li>Thoroughly test software components.</li>\n    </ul>\n\n<blockquote class=\"wp-block-quote is-style-default\"> <p>Special thanks to <strong><a href=\"https://www.linkedin.com/in/gauri-tomar-005048268/\">Gauri Tomar</a></strong> for contributing to this article on takeUforward. If you also wish to share your knowledge with the takeUforward fam, <strong><a href=\"https://takeuforward.org/interviews/paid-work-from-home-internship-at-takeuforward/\">please check out this article</a></strong>.</p> </blockquote>\n"
      }
    ]
  }
]